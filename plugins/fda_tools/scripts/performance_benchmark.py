#!/usr/bin/env python3
"""
FDA-197: PostgreSQL Performance Benchmarking Suite

Comprehensive performance testing for PostgreSQL offline database:
1. Query Performance: PostgreSQL JSONB vs JSON file scanning (target: 50-200x faster)
2. Concurrent Access: 20+ parallel agents without deadlocks (target: <10ms latency)
3. Bulk Import: COPY vs INSERT for 10K records (target: 100x faster)
4. Storage Efficiency: JSONB compression with TOAST (target: 40-60% reduction)

Agent Assignments (Dual-Assignment Model):
- Assignee: voltagent-qa-sec:performance-engineer
- Delegate: voltagent-infra:database-administrator
- Reviewers: voltagent-qa-sec:test-automator, voltagent-lang:python-pro

Usage:
    python3 performance_benchmark.py --all
    python3 performance_benchmark.py --test query
    python3 performance_benchmark.py --test concurrent --agents 20
    python3 performance_benchmark.py --test bulk-import --records 10000
    python3 performance_benchmark.py --generate-report
"""

import argparse
import json
import multiprocessing
import os
import random
import statistics
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

try:
    from fda_tools.lib.postgres_database import PostgreSQLDatabase
    from fda_tools.scripts.fda_api_client import FDAClient
    _MODULES_AVAILABLE = True
except ImportError:
    _MODULES_AVAILABLE = False
    print("ERROR: Required modules not available")
    print("Ensure FDA-191 and FDA-193 are completed first")
    sys.exit(1)


class PerformanceBenchmark:
    """
    Comprehensive performance benchmarking suite for PostgreSQL offline database.
    
    Tests:
    1. Query performance (PostgreSQL vs JSON files)
    2. Concurrent access (parallel agents)
    3. Bulk import (COPY vs INSERT)
    4. Storage efficiency (JSONB compression)
    """
    
    def __init__(
        self,
        postgres_host: str = "localhost",
        postgres_port: int = 6432,
        json_cache_dir: Path = None,
        output_dir: Path = None
    ):
        """Initialize performance benchmark suite."""
        self.postgres_host = postgres_host
        self.postgres_port = postgres_port
        self.json_cache_dir = json_cache_dir or Path.home() / "fda-510k-data" / "api_cache"
        self.output_dir = output_dir or Path.cwd() / "benchmark_results"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize database connection
        try:
            self.db = PostgreSQLDatabase(
                host=postgres_host,
                port=postgres_port,
                pool_size=25  # Support concurrent testing
            )
            print(f"âœ… Connected to PostgreSQL at {postgres_host}:{postgres_port}")
        except Exception as e:
            raise RuntimeError(f"Failed to connect to PostgreSQL: {e}")
        
        # Initialize API client for JSON comparison
        self.api_client = FDAClient(use_postgres=False)
        
        # Results storage
        self.results: Dict[str, Dict] = {}
    
    def benchmark_query_performance(
        self,
        sample_size: int = 100
    ) -> Dict[str, Any]:
        """
        Benchmark query performance: PostgreSQL JSONB vs JSON file scanning.
        
        Target: 50-200x faster with PostgreSQL
        
        Args:
            sample_size: Number of queries to benchmark
            
        Returns:
            Dict with timing results and speedup factor
        """
        print(f"\nðŸ“Š Benchmarking Query Performance ({sample_size} queries)...")
        
        # Get sample K-numbers from PostgreSQL
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(f"""
                    SELECT k_number 
                    FROM fda_510k 
                    ORDER BY RANDOM() 
                    LIMIT {sample_size}
                """)
                k_numbers = [row[0] for row in cur.fetchall()]
        
        if not k_numbers:
            return {
                "error": "No data in PostgreSQL database for benchmarking",
                "suggestion": "Run migration first: python3 migrate_to_postgres.py"
            }
        
        print(f"  Testing with {len(k_numbers)} K-numbers...")
        
        # Benchmark 1: PostgreSQL JSONB queries
        print("  [1/2] PostgreSQL JSONB queries...")
        postgres_times = []
        
        for k_number in k_numbers:
            start = time.perf_counter()
            result = self.db.get_record("510k", k_number)
            elapsed = time.perf_counter() - start
            postgres_times.append(elapsed * 1000)  # Convert to ms
        
        # Benchmark 2: JSON file scanning
        print("  [2/2] JSON file scanning...")
        json_times = []
        
        for k_number in k_numbers:
            start = time.perf_counter()
            # Scan JSON files for matching K-number
            found = False
            for json_file in self.json_cache_dir.glob("*.json"):
                try:
                    with open(json_file) as f:
                        data = json.load(f)
                    
                    # Check if this file contains the K-number
                    if "results" in data.get("data", {}):
                        for result in data["data"]["results"]:
                            if result.get("k_number") == k_number:
                                found = True
                                break
                    if found:
                        break
                except:
                    continue
            
            elapsed = time.perf_counter() - start
            json_times.append(elapsed * 1000)  # Convert to ms
        
        # Calculate statistics
        results = {
            "test": "query_performance",
            "sample_size": len(k_numbers),
            "postgres": {
                "mean_ms": statistics.mean(postgres_times),
                "median_ms": statistics.median(postgres_times),
                "min_ms": min(postgres_times),
                "max_ms": max(postgres_times),
                "stdev_ms": statistics.stdev(postgres_times) if len(postgres_times) > 1 else 0,
            },
            "json": {
                "mean_ms": statistics.mean(json_times),
                "median_ms": statistics.median(json_times),
                "min_ms": min(json_times),
                "max_ms": max(json_times),
                "stdev_ms": statistics.stdev(json_times) if len(json_times) > 1 else 0,
            },
            "speedup": {
                "mean": statistics.mean(json_times) / statistics.mean(postgres_times),
                "median": statistics.median(json_times) / statistics.median(postgres_times),
            },
            "target_met": statistics.mean(json_times) / statistics.mean(postgres_times) >= 50,
        }
        
        print(f"  âœ… PostgreSQL: {results['postgres']['mean_ms']:.2f}ms avg")
        print(f"  âœ… JSON Files: {results['json']['mean_ms']:.2f}ms avg")
        print(f"  âœ… Speedup: {results['speedup']['mean']:.1f}x faster")
        print(f"  {'âœ…' if results['target_met'] else 'âŒ'} Target (50-200x): {'MET' if results['target_met'] else 'NOT MET'}")
        
        self.results["query_performance"] = results
        return results
    
    def benchmark_concurrent_access(
        self,
        num_agents: int = 20,
        queries_per_agent: int = 50
    ) -> Dict[str, Any]:
        """
        Benchmark concurrent access: 20+ parallel agents without deadlocks.
        
        Target: <10ms query latency with 20+ agents
        
        Args:
            num_agents: Number of concurrent agents
            queries_per_agent: Queries each agent executes
            
        Returns:
            Dict with concurrency results
        """
        print(f"\nðŸ“Š Benchmarking Concurrent Access ({num_agents} agents, {queries_per_agent} queries each)...")
        
        # Get sample K-numbers
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT k_number FROM fda_510k LIMIT 100")
                k_numbers = [row[0] for row in cur.fetchall()]
        
        if not k_numbers:
            return {"error": "No data in database"}
        
        def agent_query_worker(agent_id: int) -> Dict:
            """Worker function for concurrent agent."""
            agent_db = PostgreSQLDatabase(
                host=self.postgres_host,
                port=self.postgres_port,
                pool_size=2
            )
            
            times = []
            errors = 0
            deadlocks = 0
            
            for _ in range(queries_per_agent):
                k_number = random.choice(k_numbers)
                start = time.perf_counter()
                
                try:
                    result = agent_db.get_record("510k", k_number)
                    elapsed = time.perf_counter() - start
                    times.append(elapsed * 1000)
                except Exception as e:
                    errors += 1
                    if "deadlock" in str(e).lower():
                        deadlocks += 1
            
            agent_db.close()
            
            return {
                "agent_id": agent_id,
                "queries": len(times),
                "errors": errors,
                "deadlocks": deadlocks,
                "mean_ms": statistics.mean(times) if times else 0,
                "max_ms": max(times) if times else 0,
            }
        
        # Execute concurrent queries
        print(f"  Starting {num_agents} parallel agents...")
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=num_agents) as executor:
            futures = [
                executor.submit(agent_query_worker, agent_id)
                for agent_id in range(num_agents)
            ]
            
            agent_results = [f.result() for f in as_completed(futures)]
        
        total_time = time.perf_counter() - start_time
        
        # Aggregate results
        all_mean_times = [r["mean_ms"] for r in agent_results]
        total_queries = sum(r["queries"] for r in agent_results)
        total_errors = sum(r["errors"] for r in agent_results)
        total_deadlocks = sum(r["deadlocks"] for r in agent_results)
        
        results = {
            "test": "concurrent_access",
            "num_agents": num_agents,
            "queries_per_agent": queries_per_agent,
            "total_queries": total_queries,
            "total_time_seconds": total_time,
            "queries_per_second": total_queries / total_time,
            "mean_latency_ms": statistics.mean(all_mean_times),
            "max_latency_ms": max(r["max_ms"] for r in agent_results),
            "total_errors": total_errors,
            "total_deadlocks": total_deadlocks,
            "error_rate": total_errors / total_queries if total_queries > 0 else 0,
            "target_met": statistics.mean(all_mean_times) < 10 and total_deadlocks == 0,
        }
        
        print(f"  âœ… Total Queries: {total_queries} in {total_time:.2f}s")
        print(f"  âœ… Throughput: {results['queries_per_second']:.1f} queries/second")
        print(f"  âœ… Mean Latency: {results['mean_latency_ms']:.2f}ms")
        print(f"  âœ… Deadlocks: {total_deadlocks}")
        print(f"  {'âœ…' if results['target_met'] else 'âŒ'} Target (<10ms, no deadlocks): {'MET' if results['target_met'] else 'NOT MET'}")
        
        self.results["concurrent_access"] = results
        return results
    
    def benchmark_bulk_import(
        self,
        num_records: int = 10000
    ) -> Dict[str, Any]:
        """
        Benchmark bulk import: COPY vs INSERT for 10K records.
        
        Target: 100x faster with COPY
        
        Args:
            num_records: Number of records to import
            
        Returns:
            Dict with import timing results
        """
        print(f"\nðŸ“Š Benchmarking Bulk Import ({num_records} records)...")
        
        # Generate synthetic test data
        print(f"  Generating {num_records} synthetic records...")
        test_records = []
        for i in range(num_records):
            test_records.append({
                "k_number": f"K999{i:06d}",
                "product_code": "TEST",
                "device_name": f"Test Device {i}",
                "openfda_json": {"test": True, "index": i},
                "checksum": f"test_checksum_{i}",
            })
        
        # Benchmark 1: Individual INSERT statements
        print("  [1/2] Individual INSERT statements...")
        insert_start = time.perf_counter()
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                for record in test_records[:min(100, num_records)]:  # Limit to 100 for INSERT
                    cur.execute("""
                        INSERT INTO fda_510k (k_number, product_code, device_name, openfda_json, checksum)
                        VALUES (%s, %s, %s, %s, %s)
                        ON CONFLICT (k_number) DO UPDATE SET
                            device_name = EXCLUDED.device_name
                    """, (
                        record["k_number"],
                        record["product_code"],
                        record["device_name"],
                        json.dumps(record["openfda_json"]),
                        record["checksum"]
                    ))
                conn.commit()
        
        insert_time = time.perf_counter() - insert_start
        insert_rate = min(100, num_records) / insert_time
        
        # Benchmark 2: PostgreSQL COPY (bulk import)
        print("  [2/2] PostgreSQL COPY (bulk import)...")
        copy_start = time.perf_counter()
        
        # Use bulk_import_from_json method
        # (Simulated - actual implementation would use COPY)
        # For now, use fast INSERT with executemany
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                data = [
                    (r["k_number"], r["product_code"], r["device_name"],
                     json.dumps(r["openfda_json"]), r["checksum"])
                    for r in test_records
                ]
                
                # Use execute_values for fast batch insert (similar to COPY)
                from psycopg2.extras import execute_values
                execute_values(
                    cur,
                    """
                    INSERT INTO fda_510k (k_number, product_code, device_name, openfda_json, checksum)
                    VALUES %s
                    ON CONFLICT (k_number) DO UPDATE SET
                        device_name = EXCLUDED.device_name
                    """,
                    data,
                    page_size=1000
                )
                conn.commit()
        
        copy_time = time.perf_counter() - copy_start
        copy_rate = num_records / copy_time
        
        # Clean up test records
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("DELETE FROM fda_510k WHERE product_code = 'TEST'")
                conn.commit()
        
        # Calculate results
        speedup = insert_time / copy_time if copy_time > 0 else 0
        
        results = {
            "test": "bulk_import",
            "num_records": num_records,
            "insert": {
                "records": min(100, num_records),
                "time_seconds": insert_time,
                "records_per_second": insert_rate,
            },
            "copy": {
                "records": num_records,
                "time_seconds": copy_time,
                "records_per_second": copy_rate,
            },
            "speedup": speedup,
            "target_met": speedup >= 100,
        }
        
        print(f"  âœ… INSERT: {insert_rate:.1f} records/second")
        print(f"  âœ… COPY: {copy_rate:.1f} records/second")
        print(f"  âœ… Speedup: {speedup:.1f}x faster")
        print(f"  {'âœ…' if results['target_met'] else 'âŒ'} Target (100x): {'MET' if results['target_met'] else 'NOT MET'}")
        
        self.results["bulk_import"] = results
        return results
    
    def benchmark_storage_efficiency(self) -> Dict[str, Any]:
        """
        Benchmark storage efficiency: JSONB compression with TOAST.
        
        Target: 40-60% reduction vs raw JSON
        
        Returns:
            Dict with storage metrics
        """
        print("\nðŸ“Š Benchmarking Storage Efficiency...")
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                # Get table sizes
                cur.execute("""
                    SELECT
                        pg_size_pretty(pg_total_relation_size('fda_510k')) as total_size,
                        pg_size_pretty(pg_relation_size('fda_510k')) as table_size,
                        pg_size_pretty(pg_total_relation_size('fda_510k') - pg_relation_size('fda_510k')) as toast_size,
                        pg_total_relation_size('fda_510k') as total_bytes,
                        pg_relation_size('fda_510k') as table_bytes
                """)
                sizes = cur.fetchone()
                
                # Get row count
                cur.execute("SELECT COUNT(*) FROM fda_510k")
                row_count = cur.fetchone()[0]
                
                # Estimate raw JSON size (sample calculation)
                cur.execute("""
                    SELECT AVG(LENGTH(openfda_json::text))
                    FROM fda_510k
                    LIMIT 1000
                """)
                avg_json_size = cur.fetchone()[0] or 0
        
        # Calculate storage metrics
        total_bytes = sizes[3]
        estimated_raw_bytes = row_count * avg_json_size
        compression_ratio = (1 - (total_bytes / estimated_raw_bytes)) * 100 if estimated_raw_bytes > 0 else 0
        
        results = {
            "test": "storage_efficiency",
            "row_count": row_count,
            "total_size": sizes[0],
            "table_size": sizes[1],
            "toast_size": sizes[2],
            "total_bytes": total_bytes,
            "estimated_raw_bytes": estimated_raw_bytes,
            "compression_ratio_percent": compression_ratio,
            "bytes_per_record": total_bytes / row_count if row_count > 0 else 0,
            "target_met": 40 <= compression_ratio <= 60,
        }
        
        print(f"  âœ… Total Size: {sizes[0]}")
        print(f"  âœ… Table Size: {sizes[1]}")
        print(f"  âœ… TOAST Size: {sizes[2]}")
        print(f"  âœ… Rows: {row_count:,}")
        print(f"  âœ… Compression: {compression_ratio:.1f}% reduction")
        print(f"  {'âœ…' if results['target_met'] else 'âŒ'} Target (40-60%): {'MET' if results['target_met'] else 'NOT MET'}")
        
        self.results["storage_efficiency"] = results
        return results
    
    def generate_report(self) -> str:
        """Generate comprehensive performance report."""
        print("\nðŸ“ Generating Performance Report...")
        
        report_file = self.output_dir / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        report = f"""# PostgreSQL Performance Benchmark Report

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Database:** {self.postgres_host}:{self.postgres_port}  
**Benchmark Suite:** FDA-197

## Executive Summary

"""
        
        # Add summary table
        if self.results:
            report += "| Test | Status | Target | Result |\n"
            report += "|------|--------|--------|--------|\n"
            
            for test_name, result in self.results.items():
                status = "âœ… PASS" if result.get("target_met", False) else "âŒ FAIL"
                report += f"| {test_name.replace('_', ' ').title()} | {status} | See below | See below |\n"
            
            report += "\n"
        
        # Add detailed results
        for test_name, result in self.results.items():
            report += f"## {test_name.replace('_', ' ').title()}\n\n"
            report += f"```json\n{json.dumps(result, indent=2)}\n```\n\n"
        
        # Save report
        with open(report_file, "w") as f:
            f.write(report)
        
        print(f"  âœ… Report saved: {report_file}")
        
        # Also save JSON results
        json_file = self.output_dir / f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"  âœ… JSON results: {json_file}")
        
        return str(report_file)
    
    def run_all_benchmarks(self) -> Dict[str, Any]:
        """Run all performance benchmarks."""
        print("=" * 60)
        print("PostgreSQL Performance Benchmark Suite (FDA-197)")
        print("=" * 60)
        
        # Run all tests
        self.benchmark_query_performance(sample_size=100)
        self.benchmark_concurrent_access(num_agents=20, queries_per_agent=50)
        self.benchmark_bulk_import(num_records=10000)
        self.benchmark_storage_efficiency()
        
        # Generate report
        report_file = self.generate_report()
        
        print("\n" + "=" * 60)
        print("Benchmark Complete!")
        print(f"Report: {report_file}")
        print("=" * 60)
        
        return self.results
    
    def close(self):
        """Clean up resources."""
        if self.db:
            self.db.close()


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="FDA-197: PostgreSQL Performance Benchmarking"
    )
    parser.add_argument(
        "--all", action="store_true",
        help="Run all benchmarks"
    )
    parser.add_argument(
        "--test", choices=["query", "concurrent", "bulk-import", "storage"],
        help="Run specific benchmark test"
    )
    parser.add_argument(
        "--postgres-host", default="localhost",
        help="PostgreSQL/PgBouncer host (default: localhost)"
    )
    parser.add_argument(
        "--postgres-port", type=int, default=6432,
        help="PostgreSQL/PgBouncer port (default: 6432)"
    )
    parser.add_argument(
        "--agents", type=int, default=20,
        help="Number of concurrent agents (default: 20)"
    )
    parser.add_argument(
        "--records", type=int, default=10000,
        help="Number of records for bulk import test (default: 10000)"
    )
    parser.add_argument(
        "--generate-report", action="store_true",
        help="Generate report from previous results"
    )
    
    args = parser.parse_args()
    
    try:
        benchmark = PerformanceBenchmark(
            postgres_host=args.postgres_host,
            postgres_port=args.postgres_port
        )
        
        if args.all:
            benchmark.run_all_benchmarks()
        elif args.test == "query":
            benchmark.benchmark_query_performance()
            benchmark.generate_report()
        elif args.test == "concurrent":
            benchmark.benchmark_concurrent_access(num_agents=args.agents)
            benchmark.generate_report()
        elif args.test == "bulk-import":
            benchmark.benchmark_bulk_import(num_records=args.records)
            benchmark.generate_report()
        elif args.test == "storage":
            benchmark.benchmark_storage_efficiency()
            benchmark.generate_report()
        elif args.generate_report:
            benchmark.generate_report()
        else:
            print("Specify --all or --test <test_name>")
            return 1
        
        benchmark.close()
        return 0
        
    except Exception as e:
        print(f"ERROR: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
