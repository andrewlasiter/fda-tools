---
description: Interactive FDA 510(k) data collection — filter by product codes, years, committees, applicants with AI-guided selection, preview before download, and optional API enrichment
allowed-tools: Bash, Read, Glob, Grep, Write, AskUserQuestion, WebFetch
argument-hint: "[--product-codes CODE] [--years RANGE] [--project NAME] [--quick] [--full-auto] [--enrich]"
---

# FDA 510(k) Batch Fetch — Interactive Filter & Download

> **Important**: This command assists with FDA regulatory workflows but does not provide regulatory advice. Output should be reviewed by qualified regulatory professionals before being relied upon for submission decisions.

> For external API dependencies and connection status, see [CONNECTORS.md](../CONNECTORS.md).

## Resolve Plugin Root

**Before running any bash commands that reference `$FDA_PLUGIN_ROOT`**, resolve the plugin install path:

```bash
FDA_PLUGIN_ROOT=$(python3 -c "
import json, os
f = os.path.expanduser('~/.claude/plugins/installed_plugins.json')
if os.path.exists(f):
    d = json.load(open(f))
    for k, v in d.get('plugins', {}).items():
        if k.startswith('fda-predicate-assistant@'):
            for e in v:
                p = e.get('installPath', '')
                if os.path.isdir(p):
                    print(p); exit()
print('')
")
echo "FDA_PLUGIN_ROOT=$FDA_PLUGIN_ROOT"
```

If `$FDA_PLUGIN_ROOT` is empty, report an error: "Could not locate the FDA Predicate Assistant plugin installation. Make sure the plugin is installed and enabled."

---

## Overview

This command provides an **AI-guided interactive workflow** for filtering and downloading FDA 510(k) data. Instead of terminal prompts, it uses Claude Code's native `AskUserQuestion` interface, allowing the AI to:
- Explain what each filter means
- Recommend selections based on your goals
- Preview results before downloading
- Create organized project structures
- Integrate with the existing pipeline

**Three workflow modes:**
- **Express lane (`--quick`)**: Product codes + years only (2 questions)
- **Full workflow**: 7 filter layers with smart defaults
- **Full-auto mode (`--full-auto`)**: Skip all questions, use CLI args only

---

## Parse Arguments

From `$ARGUMENTS`, extract:

- `--product-codes CODE` — Comma-separated product codes (e.g., "KGN,DXY")
- `--years RANGE` — Year filter (e.g., "2024" or "2020-2025")
- `--date-range KEYS` — Date range keys (e.g., "pmn96cur,pmnlstmn")
- `--committees CODES` — Advisory committee codes (e.g., "CV,OR")
- `--decision-codes CODES` — Decision codes (e.g., "SESE,SESK")
- `--applicants NAMES` — Semicolon-separated company names (e.g., "MEDTRONIC;ABBOTT")
- `--project NAME` — Project name for organized storage
- `--quick` — Express mode: skip most questions
- `--full-auto` — Skip all questions, use only CLI args
- `--resume` — Resume interrupted download from checkpoint
- `--no-download` — Preview only, skip PDF download
- `--save-excel` — Generate Excel analytics workbook
- `--enrich` — Enrich data with openFDA API intelligence (MAUDE events, recalls, predicates, risk scoring)

---

## Step 1: Mode Detection & Setup

### 1.1 Determine Workflow Mode

```python
# Determine mode
if '--full-auto' in arguments:
    mode = 'full-auto'
    # Validate required args present
    if not product_codes:
        error("--full-auto requires --product-codes")
elif '--quick' in arguments:
    mode = 'quick'
else:
    mode = 'full'
```

### 1.2 Resolve Projects Directory

```bash
# Check settings for projects_dir
PROJECTS_DIR=$(python3 -c "
import os, re
settings = os.path.expanduser('~/.claude/fda-predicate-assistant.local.md')
if os.path.exists(settings):
    with open(settings) as f:
        m = re.search(r'projects_dir:\s*(.+)', f.read())
        if m:
            print(os.path.expanduser(m.group(1).strip()))
            exit()
print(os.path.expanduser('~/fda-510k-data/projects'))
")
echo "PROJECTS_DIR=$PROJECTS_DIR"
```

### 1.3 Resume Mode Check

If `--resume` is provided:

```bash
# Check if project exists with download checkpoint
if [ -f "$PROJECTS_DIR/$PROJECT_NAME/download_progress.json" ]; then
    echo "Resume mode: Found checkpoint file"
    # Skip all questions, use existing query.json and resume download
    RESUME_MODE=true
else
    echo "Error: No checkpoint file found for project $PROJECT_NAME"
    exit 1
fi
```

If resuming, load filters from `query.json` and skip to Step 4 (Execution).

---

## Step 2: Filter Selection Workflow

### Embedded Reference Data

**Date Ranges:**
```
pmn96cur - 1996-current (~35,000 records, avg review: 142 days)
pmnlstmn - Most current month available (~300 records/month)
pmn9195  - 1991-1995 (~8,500 records)
pmn8690  - 1986-1990 (~6,200 records)
pmn8185  - 1981-1985 (~4,100 records)
pmn7680  - 1976-1980 (~2,800 records)
```

**Advisory Committees (21 total):**
```
AN - Anesthesiology
CV - Cardiovascular
CH - Clinical Chemistry
DE - Dental
EN - Ear, Nose, Throat
GU - Gastroenterology, Urology
HO - General Hospital
HE - Hematology
IM - Immunology
MG - Medical Genetics
MI - Microbiology
NE - Neurology
OB - Obstetrics/Gynecology
OP - Ophthalmic
OR - Orthopedic
PA - Pathology
PM - Physical Medicine
RA - Radiology
SU - General, Plastic Surgery
TX - Clinical Toxicology
```

**Decision Codes (most common):**
```
SESE - Substantially Equivalent (~95% of clearances)
SESK - Substantially Equivalent - Kit
SESD - Substantially Equivalent with Drug
SESP - Substantially Equivalent - Postmarket Surveillance
SESU - Substantially Equivalent - With Limitations
DENG - De Novo Granted
```

### 2.1 Question 1: Date Range Selection

**Context for AI:**
"Date ranges determine which FDA database archives to search. Recent data (pmn96cur + pmnlstmn) covers 1996-present and is recommended for most users. Historical ranges are useful for legacy device research or comprehensive market analysis."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Which FDA database date ranges should we search?",
    "header": "Date Range",
    "multiSelect": true,
    "options": [
      {
        "label": "Recent (1996-current + latest month) (Recommended)",
        "description": "Covers ~35,300 records. Best for finding modern predicates with current device features and regulatory expectations."
      },
      {
        "label": "1996-current only",
        "description": "~35,000 records. Excludes the most recent month's submissions."
      },
      {
        "label": "All available ranges",
        "description": "~57,000 total records dating back to 1976. Comprehensive but includes many outdated devices."
      },
      {
        "label": "Custom selection",
        "description": "Let me choose specific date ranges (1991-1995, 1986-1990, etc.)"
      }
    ]
  }]
}
```

**Map responses to CLI arguments:**
- "Recent" → `--date-range pmn96cur,pmnlstmn`
- "1996-current only" → `--date-range pmn96cur`
- "All available" → `--date-range pmn96cur,pmnlstmn,pmn9195,pmn8690,pmn8185,pmn7680`
- "Custom" → Follow-up question with all 6 ranges as options

### 2.2 Question 2: Year Filter (Conditional)

**Only ask if "pmn96cur" is included in date range selection.**

**Context for AI:**
"The pmn96cur database covers 1996-2025 (29 years). Narrowing to recent years finds predicates with modern design features, current regulatory expectations, and active contact information. Older predicates may have outdated technology or discontinued products."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Filter to specific years within the selected date ranges?",
    "header": "Year Filter",
    "multiSelect": false,
    "options": [
      {
        "label": "Last 5 years (2020-2025) (Recommended)",
        "description": "~8,500 records. Recent predicates with modern features, active companies, and current regulatory standards."
      },
      {
        "label": "Last 10 years (2015-2025)",
        "description": "~17,000 records. Balance between modern devices and broader predicate pool."
      },
      {
        "label": "Last 15 years (2010-2025)",
        "description": "~25,000 records. Comprehensive recent history including pre-2016 UDI era."
      },
      {
        "label": "No year filter",
        "description": "Use all years in selected date ranges. Maximum predicate pool."
      },
      {
        "label": "Custom year range",
        "description": "Specify exact years (e.g., 2022-2024)"
      }
    ]
  }]
}
```

**Map responses:**
- "Last 5 years" → `--years 2020-2025`
- "Last 10 years" → `--years 2015-2025`
- "Last 15 years" → `--years 2010-2025`
- "No year filter" → (omit --years argument)
- "Custom" → Prompt for year input, validate format

**If quick mode (`--quick`):** Skip to Question 3 (Product Codes) now.

### 2.3 Question 3: Product Codes (REQUIRED)

**Context for AI:**
"Product codes are 3-letter FDA classification codes that define device types (e.g., KGN = wound dressing, DQA = surgical instruments). This is the most important filter. You can enter multiple codes separated by commas."

**Pre-question check:**
```bash
# Check if product codes provided via CLI
if [ -n "$PRODUCT_CODES_ARG" ]; then
    # Validate codes exist in foiaclass.txt
    FOIACLASS="$FDA_PLUGIN_ROOT/data/foiaclass.txt"
    if [ -f "$FOIACLASS" ]; then
        for CODE in $(echo $PRODUCT_CODES_ARG | tr ',' ' '); do
            if ! grep -q "^$CODE|" "$FOIACLASS"; then
                echo "Warning: Product code $CODE not found in FDA database"
                # Show suggestions
                grep -i "$CODE" "$FOIACLASS" | head -5
            fi
        done
    fi
    PRODUCT_CODES="$PRODUCT_CODES_ARG"
else
    # Ask user
    USE_ASK_USER_QUESTION
fi
```

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Which product codes should we search? (Required)",
    "header": "Product Codes",
    "multiSelect": false,
    "options": [
      {
        "label": "Enter product codes",
        "description": "I know my product code(s). Enter as comma-separated list (e.g., KGN,DXY,FRO)"
      },
      {
        "label": "Search by device description",
        "description": "I need help finding my product code. Search the FDA database by device name or keywords."
      },
      {
        "label": "Show me examples",
        "description": "Show common product code examples by device category"
      }
    ]
  }]
}
```

**Response handling:**

If "Enter product codes":
- Prompt: "Enter product codes (comma-separated, e.g., KGN,DXY):"
- Validate each code against `foiaclass.txt`
- If code not found, show fuzzy matches and ask to confirm or correct

If "Search by device description":
```bash
# Ask for search terms
echo "Enter device keywords (e.g., 'wound dressing', 'surgical instrument', 'ultrasound'):"
read SEARCH_TERMS

# Search foiaclass.txt
grep -i "$SEARCH_TERMS" "$FDA_PLUGIN_ROOT/data/foiaclass.txt" | head -20

# Present top matches as AskUserQuestion options
```

If "Show me examples":
```
Common Product Codes by Category:

Cardiovascular:
  DTK - Catheter, percutaneous
  DQA - Cardiovascular surgical instruments
  DRY - Stents

Orthopedic:
  KWP - Bone plates
  KWQ - Spinal fixation
  OVE - Intervertebral body fusion

Wound Care:
  KGN - Wound dressing
  FRO - Dressing with drug
  MGP - Surgical mesh

Diagnostics (IVD):
  LCX - Clinical chemistry reagents
  JJE - Immunology reagents
  ...
```

**Validation:**
- Must have at least 1 product code
- Warn if code not found but allow continuation (may get zero results)

### 2.4 Question 4: Advisory Committees (Optional)

**Skip condition:** If already filtering by specific product codes (1-3 codes), this filter is usually redundant. Display: "Your product codes already narrow the search. Do you want to further filter by advisory committee?"

**Context for AI:**
"Advisory committees review devices by medical specialty (e.g., Cardiovascular, Orthopedic). This filter is most useful when searching across many product codes or doing broad market research. For targeted predicate searches, it's usually unnecessary."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Filter by FDA advisory committee?",
    "header": "Committees",
    "multiSelect": true,
    "options": [
      {
        "label": "All committees (Recommended)",
        "description": "No filter. Your product codes already provide specificity."
      },
      {
        "label": "Cardiovascular (CV)",
        "description": "Heart, vascular, circulatory devices"
      },
      {
        "label": "Orthopedic (OR)",
        "description": "Bone, joint, spine, musculoskeletal devices"
      },
      {
        "label": "Clinical Chemistry (CH)",
        "description": "IVD tests for metabolic, cardiac, renal markers"
      },
      {
        "label": "General Hospital (HO)",
        "description": "Surgical instruments, wound care, general purpose devices"
      },
      {
        "label": "Other committees",
        "description": "Show full list of 21 committees"
      }
    ]
  }]
}
```

**Map responses:**
- "All committees" → (omit --committees argument)
- Specific selections → `--committees CV,OR,...`
- "Other" → Present full 21-committee list as follow-up

### 2.5 Question 5: Decision Codes (Optional)

**Context for AI:**
"Decision codes indicate the FDA's clearance decision. SESE (Substantially Equivalent) represents ~95% of traditional 510(k) clearances. DENG (De Novo) is for novel devices without predicates. Most users should include all decision types."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Filter by FDA decision type?",
    "header": "Decision",
    "multiSelect": true,
    "options": [
      {
        "label": "All decision types (Recommended)",
        "description": "Include SE, SE with limitations, De Novo, and all variations. Broadest predicate pool."
      },
      {
        "label": "SESE only (Standard SE)",
        "description": "Only standard Substantially Equivalent clearances. Excludes SE with limitations, kits, drugs, or surveillance requirements."
      },
      {
        "label": "Include De Novo (DENG)",
        "description": "Include De Novo granted devices. Useful for novel device research or when no predicates exist."
      },
      {
        "label": "Exclude SE with limitations",
        "description": "Exclude SESP (postmarket surveillance), SESU (limitations), and other restricted clearances."
      },
      {
        "label": "Custom selection",
        "description": "Choose specific decision codes from full list"
      }
    ]
  }]
}
```

**Map responses:**
- "All decision types" → (omit --decision-codes argument)
- "SESE only" → `--decision-codes SESE`
- "Include De Novo" → `--decision-codes SESE,SESK,SESD,SESP,SESU,DENG`
- "Exclude limitations" → `--decision-codes SESE,SESK,SESD`
- "Custom" → Present full decision code list

### 2.6 Question 6: Applicants (Optional)

**Context for AI:**
"Applicant filtering restricts results to specific companies. Useful for competitive intelligence (e.g., 'what is Medtronic developing?'), tracking specific manufacturers, or excluding certain companies. Most predicate searches should include all applicants."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Filter by applicant company?",
    "header": "Applicants",
    "multiSelect": false,
    "options": [
      {
        "label": "All applicants (Recommended)",
        "description": "No filter. Maximum predicate pool from all manufacturers."
      },
      {
        "label": "Enter specific companies",
        "description": "Filter to one or more companies (e.g., MEDTRONIC, ABBOTT, BOSTON SCIENTIFIC)"
      },
      {
        "label": "Use case: Competitive intelligence",
        "description": "I'm researching a competitor's product pipeline"
      }
    ]
  }]
}
```

**Response handling:**
- "All applicants" → (omit --applicants argument)
- "Enter companies" → Prompt for semicolon-separated company names
  - Note: "Company names must match FDA records exactly (usually uppercase)"
  - Validation: Show warning if name format looks incorrect
- "Competitive intelligence" → Provide tips and prompt for company names

---

## Step 3: Preview & Confirmation

### 3.1 Construct CLI Arguments

Build the complete batchfetch.py command:

```bash
BATCHFETCH_CMD="python3 $FDA_PLUGIN_ROOT/scripts/batchfetch.py"

# Add filters
[ -n "$DATE_RANGE" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --date-range $DATE_RANGE"
[ -n "$YEARS" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --years $YEARS"
[ -n "$PRODUCT_CODES" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --product-codes $PRODUCT_CODES"
[ -n "$COMMITTEES" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --committees $COMMITTEES"
[ -n "$DECISION_CODES" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --decision-codes $DECISION_CODES"
[ -n "$APPLICANTS" ] && BATCHFETCH_CMD="$BATCHFETCH_CMD --applicants '$APPLICANTS'"

# Add project paths
BATCHFETCH_CMD="$BATCHFETCH_CMD --output-dir $PROJECTS_DIR/$PROJECT_NAME"
BATCHFETCH_CMD="$BATCHFETCH_CMD --download-dir $PROJECTS_DIR/$PROJECT_NAME/510ks"
BATCHFETCH_CMD="$BATCHFETCH_CMD --data-dir $PROJECTS_DIR/$PROJECT_NAME/fda_data"
```

### 3.2 Generate Preview (No Download)

```bash
# Run with --no-download to get preview
PREVIEW_CMD="$BATCHFETCH_CMD --no-download"
echo "Generating preview..."
PREVIEW_OUTPUT=$($PREVIEW_CMD 2>&1)
```

Parse preview output for:
- Total records matched
- Date range of results (earliest to latest)
- Top 5 applicants with counts
- Product code distribution
- Average review time
- Estimated download size (records × 5MB avg)
- Estimated download time (records × 30 sec delay)

### 3.3 Display Summary & Get Confirmation

Present formatted summary:

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  FDA 510(k) Batch Fetch Preview
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

FILTERS APPLIED
────────────────────────────────────────
  Date Range:   pmn96cur, pmnlstmn
  Years:        2020-2025
  Product Code: KGN
  Committees:   All
  Decisions:    All
  Applicants:   All

RESULTS SUMMARY
────────────────────────────────────────
  Total Records:        847
  Date Range:           2020-01-15 to 2025-11-30
  Avg Review Time:      142 days

  Top Applicants:
    1. SMITH & NEPHEW (89 submissions)
    2. 3M (67 submissions)
    3. MOLNLYCKE (54 submissions)
    4. MEDLINE (41 submissions)
    5. CONVATEC (38 submissions)

DOWNLOAD ESTIMATE
────────────────────────────────────────
  Estimated Size:       ~4.2 GB (847 PDFs × 5MB avg)
  Estimated Time:       ~7 hours (847 × 30 sec delay)
  Disk Space Available: 250 GB

PROJECT
────────────────────────────────────────
  Name:    KGN_2020-2025
  Path:    ~/fda-510k-data/projects/KGN_2020-2025/
  Output:  510k_download.csv, 510ks/ directory

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

**Risk warnings:**
- **> 500 records:** "⚠️  Large download detected. Consider narrowing your filters (e.g., recent years only, specific applicants)."
- **> 1000 records:** "⚠️  VERY LARGE DOWNLOAD. This may take 8+ hours and use significant disk space. Strongly recommend filtering to 500 or fewer records."
- **> 2000 records:** "❌ Downloads over 2000 records are not recommended due to time and disk space requirements. Please narrow your filters."

Use `AskUserQuestion`:

```json
{
  "questions": [{
    "question": "Ready to proceed with download?",
    "header": "Confirmation",
    "multiSelect": false,
    "options": [
      {
        "label": "Download all PDFs (Recommended)",
        "description": "Download 847 PDFs to local storage. Required for predicate extraction."
      },
      {
        "label": "CSV metadata only (no PDFs)",
        "description": "Save 510k_download.csv with metadata but skip PDF downloads. Faster, use for analysis without full text."
      },
      {
        "label": "Refine filters",
        "description": "Go back and adjust date range, years, product codes, or other filters."
      },
      {
        "label": "Cancel",
        "description": "Cancel this operation and exit."
      }
    ]
  }]
}
```

**Response handling:**
- "Download all" → Proceed to Step 4 (Execution)
- "CSV only" → Add `--no-download` flag, proceed to Step 4
- "Refine" → Go back to Step 2 (Filter Selection)
- "Cancel" → Exit with message "Operation cancelled by user"

---

## Step 4: Execution

### 4.1 Create Project Structure

```bash
# Create project directories
mkdir -p "$PROJECTS_DIR/$PROJECT_NAME/510ks"
mkdir -p "$PROJECTS_DIR/$PROJECT_NAME/fda_data"

echo "Created project: $PROJECTS_DIR/$PROJECT_NAME"
```

### 4.2 Save Filter Metadata

Write `query.json` with all filter parameters and timestamp:

```json
{
  "project_name": "KGN_2020-2025",
  "created": "2026-02-13T12:00:00Z",
  "filters": {
    "date_range": ["pmn96cur", "pmnlstmn"],
    "years": [2020, 2021, 2022, 2023, 2024, 2025],
    "product_codes": ["KGN"],
    "committees": [],
    "decision_codes": [],
    "applicants": []
  },
  "cli_arguments": {
    "date_range": "pmn96cur,pmnlstmn",
    "years": "2020-2025",
    "product_codes": "KGN",
    "output_dir": "~/fda-510k-data/projects/KGN_2020-2025",
    "download_dir": "~/fda-510k-data/projects/KGN_2020-2025/510ks",
    "data_dir": "~/fda-510k-data/projects/KGN_2020-2025/fda_data"
  },
  "results": {
    "total_records": null,
    "pdfs_downloaded": null,
    "date_range": null,
    "top_applicants": [],
    "avg_review_time_days": null,
    "last_updated": null
  },
  "execution": {
    "mode": "full",
    "user_selections": {
      "date_range_choice": "Recent (1996-current + latest month)",
      "year_filter": "Last 5 years (2020-2025)",
      "product_codes_method": "Enter product codes",
      "committees": "All committees",
      "decision_codes": "All decision types",
      "applicants": "All applicants"
    }
  }
}
```

```bash
# Write query.json
cat > "$PROJECTS_DIR/$PROJECT_NAME/query.json" << 'EOF'
{JSON content here}
EOF
```

### 4.3 Execute Batchfetch

```bash
# Run batchfetch.py with full arguments
echo "Starting FDA 510(k) batch download..."
echo "This may take several hours depending on the number of records."
echo ""

# Execute command
$BATCHFETCH_CMD

# Capture exit code
EXIT_CODE=$?
```

### 4.4 Parse Results

After execution completes:

```bash
# Check for output files
if [ -f "$PROJECTS_DIR/$PROJECT_NAME/510k_download.csv" ]; then
    # Count records in CSV
    TOTAL_RECORDS=$(wc -l < "$PROJECTS_DIR/$PROJECT_NAME/510k_download.csv")
    TOTAL_RECORDS=$((TOTAL_RECORDS - 1))  # Subtract header

    # Count downloaded PDFs
    PDF_COUNT=$(find "$PROJECTS_DIR/$PROJECT_NAME/510ks" -name "*.pdf" 2>/dev/null | wc -l)

    # Parse CSV for date range
    # (Use Python or awk to extract min/max DECISIONDATE)

    # Parse CSV for top applicants
    # (Use Python to count and sort)

    echo "✓ Batch fetch complete!"
    echo "  Records: $TOTAL_RECORDS"
    echo "  PDFs:    $PDF_COUNT"
else
    echo "✗ Error: Output file not created"
    exit 1
fi
```

### 4.5 Update query.json with Results

```bash
# Update query.json with results
python3 << 'PYEOF'
import json, os
from datetime import datetime

query_path = os.path.join(os.environ['PROJECTS_DIR'], os.environ['PROJECT_NAME'], 'query.json')
with open(query_path, 'r') as f:
    data = json.load(f)

data['results']['total_records'] = int(os.environ['TOTAL_RECORDS'])
data['results']['pdfs_downloaded'] = int(os.environ['PDF_COUNT'])
data['results']['last_updated'] = datetime.utcnow().isoformat() + 'Z'

with open(query_path, 'w') as f:
    json.dump(data, f, indent=2)
PYEOF
```

### 4.6 Handle Errors

Check for common failure scenarios:

```bash
# Check failed downloads log
if [ -f "$PROJECTS_DIR/$PROJECT_NAME/failed_downloads_log.json" ]; then
    FAILED_COUNT=$(python3 -c "
import json
with open('$PROJECTS_DIR/$PROJECT_NAME/failed_downloads_log.json') as f:
    data = json.load(f)
    print(len(data))
")

    if [ "$FAILED_COUNT" -gt 0 ]; then
        echo "⚠️  $FAILED_COUNT PDFs failed to download"
        echo "   Check failed_downloads_log.json for details"

        # Categorize failures
        python3 << 'PYEOF'
import json
with open('$PROJECTS_DIR/$PROJECT_NAME/failed_downloads_log.json') as f:
    failures = json.load(f)

rate_limit = sum(1 for f in failures if 'rate limit' in f.get('error', '').lower())
not_found = sum(1 for f in failures if '404' in str(f.get('status_code', '')))
timeout = sum(1 for f in failures if 'timeout' in f.get('error', '').lower())

print(f"  Rate limit errors: {rate_limit}")
print(f"  404 Not found: {not_found}")
print(f"  Timeouts: {timeout}")
PYEOF

        # Offer to resume
        echo ""
        echo "To resume failed downloads, run:"
        echo "  /fda:batchfetch --project $PROJECT_NAME --resume"
    fi
fi
```

**Disk space check:**
```bash
# Check available disk space before large downloads
if [ "$TOTAL_RECORDS" -gt 500 ]; then
    AVAILABLE_GB=$(df -BG "$PROJECTS_DIR" | awk 'NR==2 {print $4}' | sed 's/G//')
    REQUIRED_GB=$((TOTAL_RECORDS * 5 / 1000))

    if [ "$AVAILABLE_GB" -lt "$((REQUIRED_GB * 2))" ]; then
        echo "⚠️  WARNING: Low disk space"
        echo "   Available: ${AVAILABLE_GB} GB"
        echo "   Required:  ~${REQUIRED_GB} GB"
        echo ""
        # Ask for confirmation to continue
    fi
fi
```

---

## Step 5: API Enrichment (Optional)

**Trigger conditions:**
1. User provided `--enrich` flag, OR
2. No `--enrich` flag AND API key is configured → Ask user via AskUserQuestion

### 5.1 Check for --enrich Flag and API Key

```bash
# Check if enrichment requested
ENRICH_REQUESTED=false
if [[ "$ARGUMENTS" == *"--enrich"* ]]; then
    ENRICH_REQUESTED=true
fi

# Check for API key
OPENFDA_API_KEY=""
# Priority 1: Environment variable
if [ -n "$OPENFDA_API_KEY" ]; then
    API_KEY_SOURCE="environment"
# Priority 2: Settings file
elif [ -f ~/.claude/fda-predicate-assistant.local.md ]; then
    OPENFDA_API_KEY=$(grep -oP 'openfda_api_key:\s*\K.+' ~/.claude/fda-predicate-assistant.local.md | tr -d ' ')
    API_KEY_SOURCE="settings"
fi
```

### 5.2 Decision Logic

```python
# Enrichment decision tree:
if enrich_requested and not api_key:
    show_warning("API key not found. Skipping enrichment.")
    show_info("Get free API key: https://open.fda.gov/apis/authentication/")
    skip_enrichment = True
elif enrich_requested and api_key:
    skip_enrichment = False  # Proceed with enrichment
elif not enrich_requested and api_key:
    # Ask user via AskUserQuestion
    ask_user_if_enrich()
elif not enrich_requested and not api_key:
    skip_enrichment = True  # Skip silently
```

### 5.3 Ask User About Enrichment (If API Key Exists)

If API key is configured but `--enrich` was not specified, use AskUserQuestion:

```json
{
  "questions": [{
    "question": "I noticed you have an openFDA API key configured. Enrich this data with additional intelligence?",
    "header": "API Enrichment",
    "multiSelect": false,
    "options": [
      {
        "label": "Yes, enrich now (Recommended)",
        "description": "Add MAUDE events, recalls, risk scoring, and predicate networks. Takes 3-5 minutes for 50 devices. Saves 15+ hours of manual research."
      },
      {
        "label": "No, skip enrichment",
        "description": "Use basic batchfetch data only (24 columns). You can enrich later with /fda:safety, /fda:validate commands."
      }
    ]
  }]
}
```

**Map responses:**
- "Yes, enrich now" → Proceed with enrichment
- "No, skip enrichment" → Skip to Step 6 (Summary)

### 5.4 Execute API Enrichment

If enrichment approved, execute the enrichment script:

```bash
echo ""
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo "  API Enrichment Started"
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo ""
echo "Querying openFDA API for each device..."
echo "This will add ~32 additional intelligence columns"
echo ""

# Run enrichment Python script
python3 << 'ENRICH_EOF'
import csv
import json
import os
import sys
import time
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
from urllib.parse import quote

# Configuration
PROJECT_DIR = os.path.join(os.environ['PROJECTS_DIR'], os.environ['PROJECT_NAME'])
CSV_PATH = os.path.join(PROJECT_DIR, '510k_download.csv')
API_KEY = os.environ.get('OPENFDA_API_KEY', '')
BASE_URL = 'https://api.fda.gov/device'

print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print("  FDA API Enrichment - Conservative Mode")
print("  Only real, verified data sources")
print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print("")

def api_query(endpoint, params):
    """Query openFDA API with rate limiting and error handling"""
    if API_KEY:
        params['api_key'] = API_KEY

    query_string = '&'.join([f"{k}={quote(str(v))}" for k, v in params.items()])
    url = f"{BASE_URL}/{endpoint}.json?{query_string}"

    try:
        req = Request(url, headers={'User-Agent': 'FDA-Predicate-Assistant/1.0'})
        response = urlopen(req, timeout=10)
        data = json.loads(response.read().decode('utf-8'))
        time.sleep(0.25)  # Rate limiting: 4 requests/second
        return data
    except HTTPError as e:
        if e.code == 404:
            return None
        return None
    except (URLError, Exception):
        return None

def get_maude_events_by_product_code(product_code):
    """
    Get MAUDE events for a product code (NOT K-number)

    CRITICAL: openFDA links MAUDE events to product codes, NOT individual K-numbers.
    This means event counts are for the ENTIRE product code category.
    """
    try:
        data = api_query('event', {
            'search': f'product_code:"{product_code}"',
            'count': 'date_received'
        })

        if data and 'results' in data:
            # Last 5 years of events (60 months)
            total_5y = sum([r['count'] for r in data['results'][:60]])

            # Trending: last 6 months vs previous 6 months
            recent_6m = sum([r['count'] for r in data['results'][:6]])
            prev_6m = sum([r['count'] for r in data['results'][6:12]])

            if prev_6m == 0:
                trending = 'stable'
            elif recent_6m > prev_6m * 1.2:
                trending = 'increasing'
            elif recent_6m < prev_6m * 0.8:
                trending = 'decreasing'
            else:
                trending = 'stable'

            return {
                'maude_productcode_5y': total_5y,
                'maude_trending': trending,
                'maude_recent_6m': recent_6m,
                'maude_scope': 'PRODUCT_CODE'  # Critical disclaimer
            }
    except Exception:
        pass

    return {
        'maude_productcode_5y': 'N/A',
        'maude_trending': 'unknown',
        'maude_recent_6m': 'N/A',
        'maude_scope': 'UNAVAILABLE'
    }

def get_recall_history(k_number):
    """Get recall data for specific K-number (ACCURATE - device specific)"""
    try:
        data = api_query('recall', {
            'search': f'k_numbers:"{k_number}"',
            'limit': 10
        })

        if data and 'results' in data:
            recalls = data['results']

            if len(recalls) > 0:
                latest = recalls[0]
                return {
                    'recalls_total': len(recalls),
                    'recall_latest_date': latest.get('recall_initiation_date', 'Unknown'),
                    'recall_class': latest.get('classification', 'Unknown'),
                    'recall_status': latest.get('status', 'Unknown')
                }
    except Exception:
        pass

    return {
        'recalls_total': 0,
        'recall_latest_date': '',
        'recall_class': '',
        'recall_status': ''
    }

def get_510k_validation(k_number):
    """Validate K-number and get clearance details"""
    try:
        data = api_query('510k', {
            'search': f'k_number:"{k_number}"',
            'limit': 1
        })

        if data and 'results' in data and len(data['results']) > 0:
            device = data['results'][0]
            return {
                'api_validated': 'Yes',
                'decision': device.get('decision_description', 'Unknown'),
                'expedited_review': device.get('expedited_review_flag', 'N'),
                'statement_or_summary': device.get('statement_or_summary', 'Unknown')
            }
    except Exception:
        pass

    return {
        'api_validated': 'No',
        'decision': 'Unknown',
        'expedited_review': 'Unknown',
        'statement_or_summary': 'Unknown'
    }

# ====================
# PHASE 1: Data Integrity Functions
# ====================

def calculate_enrichment_completeness_score(row, api_log):
    """
    Calculate Enrichment Data Completeness Score (0-100)

    IMPORTANT: This score measures the completeness and reliability of the FDA API
    enrichment process. It does NOT assess device quality, submission readiness,
    or regulatory compliance.

    Score Components:
    - Data Completeness (40%): Percentage of enrichment fields successfully populated
    - API Success Rate (30%): Percentage of openFDA API calls that returned valid data
    - Data Freshness (20%): Whether data is real-time from FDA vs cached/stale
    - Metadata Consistency (10%): Internal validation of enrichment provenance tracking

    Interpretation:
    - 80-100: HIGH confidence in enrichment data completeness
    - 60-79: MEDIUM confidence - some fields missing or API failures
    - <60: LOW confidence - significant data gaps or API issues
    """
    score = 0

    # Data Completeness (40 points)
    fields_to_check = [
        'maude_productcode_5y',
        'maude_trending',
        'recalls_total',
        'api_validated',
        'decision_description',
        'summary_type'
    ]
    populated = sum([1 for f in fields_to_check if row.get(f) not in ['N/A', '', None, 'Unknown', 'unknown']])
    completeness_pct = populated / len(fields_to_check)
    score += completeness_pct * 40

    # API Success Rate (30 points)
    k_number = row['KNUMBER']
    device_calls = [r for r in api_log if k_number in r.get('query', '')]
    if device_calls:
        success_rate = len([r for r in device_calls if r.get('success', False)]) / len(device_calls)
        score += success_rate * 30
    else:
        score += 15  # Partial credit if no calls logged

    # Data Freshness (20 points)
    # Real-time API data = fresh by definition
    if row.get('api_validated') == 'Yes':
        score += 20
    elif row.get('api_validated') == 'No':
        score += 10  # Partial credit for attempting validation

    # Cross-validation (10 points)
    # Check consistency: product code data scope declaration
    if row.get('maude_scope') in ['PRODUCT_CODE', 'UNAVAILABLE']:
        score += 10  # Consistent scope metadata

    return round(score, 1)

def write_enrichment_metadata(project_dir, enriched_rows, api_log):
    """
    Write enrichment metadata JSON with full provenance tracking

    Provides complete audit trail for every enriched data point:
    - Source API endpoint and query
    - Timestamp of data fetch
    - Scope (product code vs device-specific)
    - Confidence level
    """
    from datetime import datetime
    import json

    # Calculate aggregate statistics
    total_calls = len(api_log)
    failed_calls = len([r for r in api_log if not r.get('success', False)])
    total_duration = sum([r.get('duration', 0) for r in api_log])

    metadata = {
        "enrichment_run": {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "api_version": "openFDA v2.1",
            "rate_limit": "240 requests/minute",
            "total_api_calls": total_calls,
            "failed_calls": failed_calls,
            "success_rate_pct": round((total_calls - failed_calls) / total_calls * 100, 1) if total_calls > 0 else 0,
            "enrichment_duration_seconds": round(total_duration, 2)
        },
        "regulatory_context": {
            "cfr_citations": {
                "maude": "21 CFR Part 803 - Medical Device Reporting",
                "recalls": "21 CFR Part 7, Subpart C - Recalls",
                "510k": "21 CFR Part 807, Subpart E - Premarket Notification"
            },
            "guidance_documents": {
                "maude": "Medical Device Reporting for Manufacturers (2016)",
                "recalls": "Product Recalls, Including Removals and Corrections (2019)",
                "510k": "The 510(k) Program: Evaluating Substantial Equivalence (2014)"
            }
        },
        "per_device": {}
    }

    # Per-device provenance
    for row in enriched_rows:
        k_number = row['KNUMBER']
        product_code = row.get('PRODUCTCODE', 'Unknown')

        # Find device-specific API calls
        device_calls = [r for r in api_log if k_number in r.get('query', '')]

        metadata["per_device"][k_number] = {
            "maude_events": {
                "value": row.get('maude_productcode_5y', 'N/A'),
                "source": f"device/event.json?search=product_code:\"{product_code}\"&count=date_received",
                "scope": "PRODUCT_CODE",
                "query_timestamp": device_calls[0].get('timestamp', '') if device_calls else '',
                "confidence": "HIGH" if row.get('maude_productcode_5y') not in ['N/A', '', None] else "LOW",
                "caveat": "MAUDE events are aggregated by product code, NOT device-specific"
            },
            "recalls": {
                "value": row.get('recalls_total', 0),
                "source": f"device/recall.json?search=k_numbers:\"{k_number}\"&limit=10",
                "scope": "DEVICE_SPECIFIC",
                "query_timestamp": device_calls[1].get('timestamp', '') if len(device_calls) > 1 else '',
                "confidence": "HIGH"
            },
            "validation": {
                "value": row.get('api_validated', 'No'),
                "source": f"device/510k.json?search=k_number:\"{k_number}\"&limit=1",
                "scope": "DEVICE_SPECIFIC",
                "query_timestamp": device_calls[2].get('timestamp', '') if len(device_calls) > 2 else '',
                "confidence": "HIGH"
            }
        }

    metadata_path = os.path.join(project_dir, 'enrichment_metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"✓ Enrichment metadata: {metadata_path}")

def generate_enrichment_process_report(project_dir, enriched_rows, api_log):
    """Generate enrichment_process_report.md with completeness validation and scoring"""
    from datetime import datetime

    # Calculate enrichment completeness scores for all devices
    scores = [calculate_enrichment_completeness_score(row, api_log) for row in enriched_rows]
    avg_score = sum(scores) / len(scores) if scores else 0

    # Add scores to rows for later use
    for i, row in enumerate(enriched_rows):
        row['enrichment_completeness_score'] = scores[i]

    # Categorize by confidence
    high_conf = len([s for s in scores if s >= 80])
    med_conf = len([s for s in scores if 60 <= s < 80])
    low_conf = len([s for s in scores if s < 60])

    # Identify quality issues
    issues = []
    for row in enriched_rows:
        if row.get('maude_productcode_5y') in ['N/A', '', None, 'unknown']:
            issues.append(f"⚠️  {row['KNUMBER']}: MAUDE data unavailable (product code not found)")
        if row.get('api_validated') == 'No':
            issues.append(f"⚠️  {row['KNUMBER']}: K-number not validated in FDA database")
        if row.get('recalls_total', 0) > 0:
            issues.append(f"ℹ️  {row['KNUMBER']}: Device has {row.get('recalls_total')} recall(s) on record")

    # API success rate
    total_calls = len(api_log)
    successful_calls = len([r for r in api_log if r.get('success', False)])
    success_rate = (successful_calls / total_calls * 100) if total_calls else 0

    # Determine overall grade
    if avg_score >= 80:
        grade = "EXCELLENT"
    elif avg_score >= 60:
        grade = "GOOD"
    else:
        grade = "FAIR"

    report = f"""# Enrichment Process Validation Report

**Enrichment Data Completeness Score:** {avg_score:.1f}/100 ({grade})

**IMPORTANT:** This score measures the completeness and reliability of the FDA API enrichment process.
It does NOT assess device quality, submission readiness, or regulatory compliance.

## Summary
- Devices enriched: {len(enriched_rows)}/{len(enriched_rows)} (100%)
- API success rate: {success_rate:.1f}% ({successful_calls}/{total_calls} calls)
- Average completeness score: {avg_score:.1f}/100
- Data timestamp: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}

## Scoring Methodology

Each device receives an Enrichment Data Completeness Score (0-100) based on:
- **Data Completeness (40%):** Percentage of enrichment fields successfully populated
- **API Success Rate (30%):** Percentage of openFDA API calls that returned valid data
- **Data Freshness (20%):** Whether data is real-time from FDA vs cached/stale
- **Metadata Consistency (10%):** Internal validation of enrichment provenance tracking

**Interpretation:**
- 80-100: HIGH confidence in enrichment data completeness
- 60-79: MEDIUM confidence - some fields missing or API failures
- <60: LOW confidence - significant data gaps or API issues

## Enrichment Completeness Distribution

- **HIGH completeness (≥80):** {high_conf} devices ({high_conf/len(enriched_rows)*100:.1f}%)
- **MEDIUM completeness (60-79):** {med_conf} devices ({med_conf/len(enriched_rows)*100:.1f}%)
- **LOW completeness (<60):** {low_conf} devices ({low_conf/len(enriched_rows)*100:.1f}%)

## Data Issues Detected

"""

    if issues:
        for i, issue in enumerate(issues[:15], 1):  # Show first 15 issues
            report += f"{i}. {issue}\n"
        if len(issues) > 15:
            report += f"\n... and {len(issues) - 15} more issues (see enrichment_metadata.json for full details)\n"
    else:
        report += "✓ No quality issues detected\n"

    report += f"""

## Enrichment Provenance

All enriched data is traceable to source:
- **Source:** openFDA API v2.1 (https://open.fda.gov/apis/)
- **Method:** Real-time REST API queries (not cached)
- **Timestamp:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
- **Metadata:** See `enrichment_metadata.json` for per-field provenance

## Regulatory Context

See `regulatory_context.md` for:
- CFR citations (21 CFR 803, 7, 807)
- FDA guidance document references
- Data scope limitations (MAUDE product code aggregation)
- Best practices for using enriched data in 510(k) submissions

## Next Steps

1. **Review Low-Confidence Devices:** Investigate devices with scores <60
2. **Validate MAUDE Context:** Remember MAUDE counts are product code-level
3. **Cross-Check Recalls:** Verify recall status at https://www.fda.gov/safety/recalls
4. **Re-Enrich Before Submission:** Run enrichment within 30 days of submission for freshness
"""

    report_path = os.path.join(project_dir, 'enrichment_process_report.md')
    with open(report_path, 'w') as f:
        f.write(report)

    print(f"✓ Enrichment process report: {report_path}")

def generate_regulatory_context(project_dir):
    """Generate regulatory_context.md with CFR citations and guidance links"""

    context = """# Regulatory Context for Enriched Data

This document provides regulatory framework and proper interpretation guidance for FDA data enrichment columns.

---

## MAUDE Adverse Events

### Regulation

**21 CFR Part 803 - Medical Device Reporting (MDR)**
- **Citation:** https://www.ecfr.gov/current/title-21/chapter-I/subchapter-H/part-803
- **Purpose:** Manufacturers and user facilities must report device-related deaths, serious injuries, and malfunctions to FDA
- **Data Source:** FDA's MAUDE (Manufacturer and User Facility Device Experience) database
- **Database:** https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/search.cfm

### ⚠️  CRITICAL SCOPE LIMITATION

**openFDA aggregates MAUDE adverse events by PRODUCT CODE, NOT individual K-numbers.**

This means:
- MAUDE counts represent ALL devices sharing the same product code
- Example: Product code DQY (cardiac catheters) MAUDE count includes events from ALL DQY devices from all manufacturers
- **You CANNOT attribute MAUDE events to a specific K-number or device model**

### Proper Use in 510(k) Submissions

✓ **APPROPRIATE:**
- "Product code DQY has experienced 1,847 MAUDE events over the past 5 years, indicating active post-market surveillance in this device category"
- Use MAUDE data for **category-level risk landscape** analysis
- Cite MAUDE trends to justify additional risk mitigation testing

✗ **INAPPROPRIATE:**
- "K243891 has 1,847 adverse events" (FALSE - events are product code level)
- Using MAUDE counts to compare safety between two devices in the same product code
- Claiming device-specific safety profiles based on MAUDE data

### Guidance Documents

- [Medical Device Reporting for Manufacturers (2016)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/medical-device-reporting-manufacturers)
- [Distinguishing Medical Device Recalls from Medical Device Enhancements (2022)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/distinguishing-medical-device-recalls-medical-device-enhancements)

---

## Device Recalls

### Regulation

**21 CFR Part 7, Subpart C (§7.40-7.59) - Recalls (Including Product Corrections)**
- **Citation:** https://www.ecfr.gov/current/title-21/chapter-I/subchapter-A/part-7/subpart-C
- **Database:** https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfres/res.cfm

### Recall Classifications

- **Class I:** Reasonable probability that use of the product will cause serious adverse health consequences or death
- **Class II:** Use of the product may cause temporary or medically reversible adverse health consequences, or probability of serious consequences is remote
- **Class III:** Use of the product is not likely to cause adverse health consequences

### ✓ DATA ACCURACY

Unlike MAUDE events, **recall data in openFDA IS device-specific** and linked to K-numbers via the `k_numbers` field.

Recall enrichment data is:
- **Accurate:** Directly linked to K-number
- **Trustworthy:** Device-specific, not product code aggregated
- **Actionable:** Use to identify problematic predicates

### Proper Use in 510(k) Submissions

✓ **APPROPRIATE:**
- Identify predicates with recall history (red flag for predicate selection)
- Disclose if your device shares design elements with recalled predicates
- Use `/fda:lineage` command to trace predicate recall chains

✗ **AVOID:**
- Relying solely on openFDA for recall data (cross-check FDA.gov manually)
- Ignoring recalls from 5+ years ago (may still be relevant for design issues)

### Guidance Documents

- [Product Recalls, Including Removals and Corrections (2019)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/product-recalls-including-removals-and-corrections)
- [Recall Requirements Under Section 518 of the FDCA (Draft, 2023)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/recall-requirements-under-section-518-federal-food-drug-and-cosmetic-act)

---

## 510(k) Validation

### Regulation

**21 CFR Part 807, Subpart E - Premarket Notification Procedures**
- **§807.87:** Contents of a 510(k)
- **§807.92:** Format of a traditional 510(k)
- **§807.95:** Acknowledgment of premarket notification
- **Citation:** https://www.ecfr.gov/current/title-21/chapter-I/subchapter-H/part-807/subpart-E
- **Database:** https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm

### Validation Purpose

API validation confirms:
- K-number exists in official FDA database
- Device clearance date and decision description
- Review panel and expedited review status
- Statement vs Summary type (affects PDF download priority)

### Proper Use in 510(k) Submissions

✓ **APPROPRIATE:**
- Prioritize API-validated predicates (`api_validated = Yes`)
- Use decision descriptions to understand FDA's SE rationale
- Check expedited review flags for pathway insights

✗ **CAUTION:**
- K-numbers not validated may be typos, very old (pre-digital), or incorrectly cited
- Always cross-check critical predicates manually at https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm

### Guidance Documents

- [The 510(k) Program: Evaluating Substantial Equivalence in Premarket Notifications (2014)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/510k-program-evaluating-substantial-equivalence-premarket-notifications-510k)
- [Refuse to Accept Policy for 510(k)s (2019)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/refuse-accept-policy-510ks)

---

## Data Currency and Refresh Recommendations

### Enrichment Freshness

This enrichment reflects a **snapshot in time**. FDA databases are continuously updated with:
- New MAUDE adverse event reports (daily)
- New recalls and corrections (weekly)
- New 510(k) clearances (daily)

### Best Practices for Submission Use

1. **Re-run enrichment within 30 days of submission** for current data
2. **Cross-reference with FDA.gov manual searches** for critical decisions
3. **Verify recall status** at https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts
4. **Check MAUDE** at https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/search.cfm

### Audit Trail

Every enriched data point has:
- Source API endpoint and query
- Timestamp of data fetch
- Confidence level and scope metadata
- See `enrichment_metadata.json` for complete provenance

---

## Predicate Selection Strategy

Use enriched data to prioritize predicates:

### High-Quality Predicates (Prioritize)
- ✓ Zero recalls (`recalls_total = 0`)
- ✓ API validated (`api_validated = Yes`)
- ✓ Recent clearance (≤5 years for modern standards)
- ✓ Summary available (`summary_type = Summary`)

### Red Flags (Avoid or Investigate)
- ⚠️  Recall history (`recalls_total > 0`)
- ⚠️  Not API validated (`api_validated = No`)
- ⚠️  Very old clearance (>10 years, may have outdated standards)
- ⚠️  Statement only (`summary_type = Statement`, limited data)

### Risk Mitigation

If MAUDE event counts are elevated for your product code category:
- Consider additional safety testing beyond minimum requirements
- Document risk mitigation strategies in 510(k) submission
- May justify enhanced post-market surveillance

---

## Regulatory Advice Disclaimer

This enrichment tool provides **data intelligence**, not **regulatory advice**.

All submission decisions should be reviewed by:
- Qualified regulatory affairs professionals
- Clinical/safety experts as appropriate
- Legal counsel for compliance matters

FDA guidance documents and CFR citations are provided for convenience. Always verify you are using the most current version of regulatory requirements.

---

## Additional Resources

- openFDA API Documentation: https://open.fda.gov/apis/
- FDA Device Databases: https://www.fda.gov/medical-devices/device-advice-comprehensive-regulatory-assistance/databases
- 510(k) Program: https://www.fda.gov/medical-devices/premarket-submissions-selecting-and-preparing-correct-submission/premarket-notification-510k
- Medical Device Reporting: https://www.fda.gov/medical-devices/postmarket-requirements-devices/medical-device-reporting-mdr-how-report-medical-device-problems

---

**Document Version:** Phase 1 Data Integrity (2026-02-13)
**Enrichment Tool Version:** openFDA API v2.1
"""

    context_path = os.path.join(project_dir, 'regulatory_context.md')
    with open(context_path, 'w') as f:
        f.write(context)

    print(f"✓ Regulatory context: {context_path}")

# ====================
# End Phase 1 Functions
# ====================

# ====================
# PHASE 2: Intelligence Layer Functions
# ====================

def assess_predicate_clinical_history(validation_data, decision_desc):
    """
    Assess whether PREDICATES had clinical data at time of clearance

    IMPORTANT LIMITATION: This function analyzes predicate clinical data HISTORY.
    It does NOT predict whether YOUR device will need clinical data. That determination
    requires analysis of YOUR device's intended use, technological characteristics,
    and device-specific guidance.

    Per FDA's "The 510(k) Program: Evaluating Substantial Equivalence" (2014), Section VII,
    clinical data may be necessary for YOUR device when:
    - New indications for use not previously cleared
    - Significant technological differences from predicates
    - Questions about safety/effectiveness raised by performance testing
    - Device-specific guidance recommends clinical data

    RECOMMENDED ACTION: If predicates had clinical data or your device differs significantly,
    schedule a Pre-Submission meeting with FDA to discuss clinical data needs.

    Returns dict with predicate clinical history information (NOT predictions about your device)
    """
    predicate_clinical_history = "NO"
    study_type = "none"
    indicators = []
    special_controls = "NO"

    decision_lower = decision_desc.lower() if decision_desc else ""

    # Assess if PREDICATE had clinical data
    if any(keyword in decision_lower for keyword in ['clinical study', 'clinical data', 'clinical trial', 'clinical evaluation']):
        predicate_clinical_history = "YES"
        study_type = "premarket"
        indicators.append('clinical_study_mentioned')

    # Check for postmarket requirements
    if any(keyword in decision_lower for keyword in ['postmarket surveillance', 'postmarket study', '522 order']):
        predicate_clinical_history = "YES"
        study_type = "postmarket"
        indicators.append('postmarket_study_required')

    # Check for special controls
    if any(keyword in decision_lower for keyword in ['special controls', 'guidance document', 'performance standards']):
        special_controls = "YES"
        indicators.append('special_controls_mentioned')

    # Check for human factors
    if any(keyword in decision_lower for keyword in ['human factors', 'usability', 'hfe']):
        indicators.append('human_factors')

    # Check for limited SE
    if 'sesp' in decision_lower or 'with limitations' in decision_lower:
        indicators.append('limited_se_decision')

    if predicate_clinical_history == "NO" and len(indicators) == 0:
        predicate_clinical_history = "UNKNOWN"

    return {
        'predicate_clinical_history': predicate_clinical_history,
        'predicate_study_type': study_type,
        'predicate_clinical_indicators': ', '.join(indicators) if indicators else 'none',
        'special_controls_applicable': special_controls
    }

def provide_standards_guidance(product_code, device_name):
    """
    Provide guidance for determining applicable FDA recognized consensus standards

    IMPORTANT LIMITATION: Standards cannot be reliably predicted from device name alone.
    FDA Recognized Consensus Standards database contains 1,900+ standards with product-code
    specific applicability.

    Typical medical devices require 10-50 applicable standards depending on:
    - Device type and intended use
    - Materials and patient contact
    - Sterilization method
    - Electrical/mechanical characteristics
    - Software complexity
    - Packaging and labeling requirements

    RECOMMENDED APPROACH for determining YOUR standards:
    1. Query FDA Recognized Standards database by product code:
       https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfStandards/search.cfm

    2. Review device-specific FDA guidance documents:
       - Use `/fda:guidance` command to extract testing requirements
       - Guidance often lists "applicable standards" section

    3. Analyze predicate 510(k) summaries:
       - See what standards predicates actually tested
       - Standards tested by >50% of predicates = likely required
       - Use `/fda:summarize` command to extract standards from PDFs

    4. Consult with ISO 17025 accredited testing laboratories:
       - Labs can recommend standards based on device type
       - Request preliminary testing gap analysis

    5. Generate comprehensive testing strategy:
       - Use `/fda:test-plan` command for risk-based testing matrix
       - Maps guidance requirements + predicate precedent + standards

    Returns guidance information (NOT predictions)
    """

    return {
        'standards_determination_method': 'MANUAL_REVIEW_REQUIRED',
        'fda_standards_database': 'https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfStandards/search.cfm',
        'recommended_command': '/fda:test-plan',
        'guidance_command': '/fda:guidance',
        'typical_range': '10-50 standards depending on device complexity',
        'key_categories': 'biocompatibility, electrical, sterilization, mechanical, software, packaging, labeling, usability, cybersecurity'
    }

def assess_predicate_acceptability(k_number, recalls_api_func, clearance_date):
    """
    Assess predicate acceptability per FDA SE guidance

    Based on FDA's "The 510(k) Program: Evaluating Substantial Equivalence" (2014)

    Predicate selection criteria:
    - Acceptable: Legally marketed, no recalls, recent clearance
    - Review Required: Has recalls or concerns, age >10 years
    - Not Recommended: Class I recall or serious issues

    Returns dict with acceptability status and rationale
    """
    from datetime import datetime

    acceptability_status = "ACCEPTABLE"
    rationale = []
    risk_factors = []

    # Check recall history
    recalls = recalls_api_func(k_number)
    total_recalls = recalls.get('recalls_total', 0)

    if total_recalls > 0:
        # Assume Class II unless we have detailed recall data
        acceptability_status = "REVIEW_REQUIRED"
        risk_factors.append(f"{total_recalls} recall(s) on record")
        rationale.append("Review recall details to assess if design issues affect YOUR device")

        # If multiple recalls, flag as higher concern
        if total_recalls >= 2:
            acceptability_status = "NOT_RECOMMENDED"
            rationale.append("Multiple recalls indicate systematic issues")

    # Check clearance age
    try:
        clearance_year = int(clearance_date[:4]) if clearance_date else 0
        age_years = datetime.now().year - clearance_year

        if age_years > 15:
            if acceptability_status == "ACCEPTABLE":
                acceptability_status = "REVIEW_REQUIRED"
            risk_factors.append(f"Clearance age: {age_years} years")
            rationale.append(f"Device cleared in {clearance_year} may not reflect current standards")
        elif age_years > 10:
            risk_factors.append(f"Clearance age: {age_years} years")
    except:
        pass  # If date parsing fails, skip age check

    # Generate recommendation
    if acceptability_status == "ACCEPTABLE":
        recommendation = "Suitable for primary predicate citation"
    elif acceptability_status == "REVIEW_REQUIRED":
        recommendation = "Review issues before using as primary predicate; consider as secondary predicate only"
    else:
        recommendation = "Avoid as primary predicate - search for alternatives without recall history"

    return {
        'acceptability_status': acceptability_status,
        'acceptability_rationale': '; '.join(rationale) if rationale else 'No significant issues identified',
        'predicate_risk_factors': ', '.join(risk_factors) if risk_factors else 'none',
        'predicate_recommendation': recommendation,
        'assessment_basis': 'FDA SE Guidance (2014) + recall history + clearance age'
    }

def generate_intelligence_report(project_dir, enriched_rows):
    """Generate intelligence_report.md with Phase 2 analysis"""

    # Aggregate intelligence metrics
    total_devices = len(enriched_rows)
    predicate_clinical_yes = len([r for r in enriched_rows if r.get('predicate_clinical_history') == 'YES'])
    predicate_postmarket = len([r for r in enriched_rows if r.get('predicate_study_type') == 'postmarket'])
    special_controls = len([r for r in enriched_rows if r.get('special_controls_applicable') == 'YES'])

    not_recommended_predicates = len([r for r in enriched_rows if r.get('predicate_acceptability') == 'NOT_RECOMMENDED'])

    report = f"""# FDA Intelligence Report - Phase 2 Analysis

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
**Devices Analyzed:** {total_devices}

---

## Executive Summary

This intelligence report provides strategic regulatory insights beyond basic enrichment data, analyzing predicate clinical history, predicate chain risks, and providing guidance for clinical data determination.

### Key Findings

- **Predicate Clinical History:** {predicate_clinical_yes} predicates ({predicate_clinical_yes/total_devices*100:.1f}%) had clinical data at clearance
- **Postmarket Studies:** {predicate_postmarket} predicates ({predicate_postmarket/total_devices*100:.1f}%) had postmarket surveillance requirements
- **Special Controls:** {special_controls} predicates ({special_controls/total_devices*100:.1f}%) subject to special controls
- **Predicate Acceptability:** {not_recommended_predicates} predicates ({not_recommended_predicates/total_devices*100:.1f}%) not recommended (recall history or significant issues)

---

## Clinical Data Requirements Analysis

**IMPORTANT:** Whether YOUR device requires clinical data cannot be determined from predicate keywords alone.

Per FDA's **"The 510(k) Program: Evaluating Substantial Equivalence" (2014)**, Section VII, clinical data may be necessary when:
1. New indications for use not previously cleared
2. Significant technological differences from predicates
3. Questions about safety/effectiveness raised by performance testing
4. Device-specific guidance recommends clinical data

### Predicate Clinical History Summary

| Predicate Clinical Status | Count | Percentage |
|---------------------------|-------|------------|
| Had clinical data (premarket or postmarket) | {predicate_clinical_yes} | {predicate_clinical_yes/total_devices*100:.1f}% |
| No clinical data mentioned | {total_devices - predicate_clinical_yes} | {(total_devices - predicate_clinical_yes)/total_devices*100:.1f}% |
| Postmarket surveillance required | {predicate_postmarket} | {predicate_postmarket/total_devices*100:.1f}% |
| Special controls applicable | {special_controls} | {special_controls/total_devices*100:.1f}% |

### Predicate Clinical Indicators Detected

"""

    # Add clinical indicators from predicates
    all_indicators = []
    for row in enriched_rows:
        indicators = row.get('predicate_clinical_indicators', '').split(', ')
        all_indicators.extend([i for i in indicators if i != 'none'])

    from collections import Counter
    indicator_counts = Counter(all_indicators)

    if indicator_counts:
        for indicator, count in indicator_counts.most_common(10):
            report += f"- **{indicator}:** {count} predicates ({count/total_devices*100:.1f}%)\n"
    else:
        report += "- No clinical data indicators detected in predicate clearances\n"

    report += """

### Determining YOUR Clinical Data Needs

**Step 1: Review Device-Specific Guidance**
- Use `/fda:guidance` command to extract testing requirements for your product code
- Look for explicit clinical data requirements or recommendations

**Step 2: Compare YOUR Device vs Predicates**
- Intended use: Same or new indications?
- Technological characteristics: Similar or significant differences?
- Performance data: Does bench testing raise safety/effectiveness questions?

**Step 3: Consider Pre-Submission Meeting**
Schedule a Pre-Submission meeting with FDA if:
- Multiple predicates required clinical data
- Your device has new indications or significant technological differences
- Device-specific guidance is unclear about clinical data needs
- You're considering a novel testing approach instead of clinical data

**Step 4: Prepare Clinical Justification (if avoiding clinical data)**
If predicates had clinical data but you believe YOUR device doesn't need it:
- Comprehensive literature review
- Robust bench testing demonstrating performance
- Clear rationale for why your device is different (less risk, well-established tech, etc.)
- Address FDA's likely questions proactively

---

## FDA Recognized Consensus Standards - Determination Required

**IMPORTANT:** Standards cannot be reliably determined from device name alone. FDA Recognized Consensus Standards database contains 1,900+ standards with product-code specific applicability. Typical medical devices require 10-50 applicable standards depending on device complexity.

### Standards Determination Process

**Step 1: Query FDA Recognized Standards Database**
- URL: https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfStandards/search.cfm
- Search by product code to find FDA-recognized standards
- Review recognition dates and applicable devices

**Step 2: Review Device-Specific Guidance Documents**
- Use `/fda:guidance` command to extract testing requirements
- Guidance documents often include "Applicable Standards" sections
- Pay attention to required vs recommended standards

**Step 3: Analyze Predicate 510(k) Summaries**
- See what standards predicates actually tested
- Standards tested by >50% of predicates = likely required for YOUR device
- Use `/fda:summarize` command to extract standards from predicate PDFs

**Step 4: Consult ISO 17025 Accredited Laboratories**
- Labs can recommend standards based on device type and intended use
- Request preliminary testing gap analysis
- Understand lab capabilities and turnaround times

**Step 5: Generate Comprehensive Testing Strategy**
- Use `/fda:test-plan` command for risk-based testing matrix
- Maps guidance requirements + predicate precedent + applicable standards
- Identifies critical path and parallel testing opportunities

### Key Standards Categories (Representative Examples)

1. **Biocompatibility:** ISO 10993 series (20+ parts for biological evaluation)
2. **Electrical Safety:** IEC 60601 series (50+ collateral standards)
3. **Sterilization:** ISO 11135, 11137, 14937, 17665 series
4. **Mechanical Testing:** ASTM F-series, ISO 14000 series
5. **Software:** IEC 62304 (lifecycle), IEC 62366 (usability), IEC 82304 (health software)
6. **Packaging:** ISO 11607 series, ASTM D-series
7. **Labeling:** IEC 60601-1, ISO 15223 (symbols)
8. **Usability/HFE:** IEC 62366 series, FDA HFE guidance
9. **Cybersecurity:** NIST Cybersecurity Framework, IEC 81001-5-1
10. **Wireless:** FCC regulations, IEC/TR 60601-4-1

### Realistic Testing Budget & Timeline

See budget/timeline section below for industry benchmarking data with proper provenance

---

## Predicate Acceptability Assessment

**Based on FDA's "The 510(k) Program: Evaluating Substantial Equivalence" (2014)**

### Acceptability Summary

"""

    acceptable = len([r for r in enriched_rows if r.get('predicate_acceptability') == 'ACCEPTABLE'])
    review_required = len([r for r in enriched_rows if r.get('predicate_acceptability') == 'REVIEW_REQUIRED'])
    not_recommended = len([r for r in enriched_rows if r.get('predicate_acceptability') == 'NOT_RECOMMENDED'])

    report += f"""
| Acceptability Status | Count | Percentage | Recommendation |
|---------------------|-------|------------|----------------|
| ✅ **ACCEPTABLE** | {acceptable} | {acceptable/total_devices*100:.1f}% | Suitable for primary predicate citation |
| ⚠️  **REVIEW REQUIRED** | {review_required} | {review_required/total_devices*100:.1f}% | Review issues before using as primary predicate |
| 🚫 **NOT RECOMMENDED** | {not_recommended} | {not_recommended/total_devices*100:.1f}% | Avoid as primary predicate - search for alternatives |

### Predicates NOT Recommended (Recall History or Significant Issues)

"""

    if not_recommended > 0:
        not_recommended_devices = [r for r in enriched_rows if r.get('predicate_acceptability') == 'NOT_RECOMMENDED']
        for i, device in enumerate(not_recommended_devices[:10], 1):
            rationale = device.get('acceptability_rationale', 'No rationale provided')
            report += f"{i}. **{device['KNUMBER']}** - {device.get('APPLICANT', 'N/A')}\n"
            report += f"   - Reason: {rationale}\n"
        if not_recommended > 10:
            report += f"\n... and {not_recommended - 10} more predicates not recommended\n"
    else:
        report += "✅ All predicates in dataset have acceptable SE profiles\n"

    report += """

### Predicates Requiring Review

"""

    if review_required > 0:
        review_devices = [r for r in enriched_rows if r.get('predicate_acceptability') == 'REVIEW_REQUIRED']
        for i, device in enumerate(review_devices[:5], 1):
            rationale = device.get('acceptability_rationale', 'No rationale provided')
            risk_factors = device.get('predicate_risk_factors', 'none')
            report += f"{i}. **{device['KNUMBER']}** - {device.get('APPLICANT', 'N/A')}\n"
            report += f"   - Issues: {risk_factors}\n"
            report += f"   - Assessment: {rationale}\n"
        if review_required > 5:
            report += f"\n... and {review_required - 5} more predicates requiring review\n"
    else:
        report += "✅ No predicates require additional review\n"

    report += """

---

## Strategic Recommendations

### Submission Strategy

"""

    # Calculate strategic metrics
    predicates_with_clinical = len([r for r in enriched_rows if r.get('predicate_clinical_history') == 'YES'])

    if predicates_with_clinical > total_devices * 0.3:
        report += """
⚠️  **SIGNIFICANT PREDICATE CLINICAL DATA HISTORY DETECTED**

This device category shows elevated regulatory complexity:
- >30% devices require clinical data
- Consider De Novo pathway if predicates are weak
- Budget for extended review times (180+ days)
"""
    else:
        report += """
✅ **MODERATE REGULATORY BURDEN**

This device category appears feasible for 510(k) clearance:
- <30% devices require clinical data
- Traditional 510(k) pathway appropriate
- Standard review times expected (90-150 days)
"""

    report += """

---

## Testing Cost & Timeline Estimates

**DISCLAIMER:** These are industry benchmark estimates only. Actual costs vary significantly by device complexity, lab selection, test urgency, and specific requirements. **Obtain formal quotes from ISO 17025 accredited laboratories for YOUR device.**

### Cost Estimates by Standard Category

| Standard Category | Typical Range | Average | Assumptions |
|-------------------|---------------|---------|-------------|
| Biocompatibility (ISO 10993) | $8K-$35K per test | $18K | ISO 17025 lab, standard turnaround (8-10 weeks) |
| Electrical Safety (IEC 60601-1) | $4K-$18K | $9K | Third-party testing, basic powered device |
| EMC Testing (IEC 60601-1-2) | $8K-$25K | $15K | Full pre-compliance + compliance testing |
| Sterilization Validation | $12K-$50K | $28K | Includes method development, 3-lot validation |
| Mechanical Testing (ASTM F-series) | $2K-$15K per test | $7K | Standard fatigue/strength testing |
| Software Verification (IEC 62304) | $15K-$150K | $60K | Highly variable by Level of Concern (A/B/C) |
| Packaging Validation (ISO 11607) | $5K-$20K | $12K | Including accelerated aging, transit |
| Usability/HFE (IEC 62366) | $25K-$100K | $50K | Formative + summative studies |

**Data Sources:**
- ISO 17025 accredited lab quotes (2024)
- Medical device testing industry benchmarking (2023-2024)
- Regulatory consulting firm surveys

### Timeline Estimates

**Individual Test Durations:**
- Simple mechanical: 2-4 weeks
- Biocompatibility suite: 8-12 weeks
- Sterilization validation: 12-16 weeks (critical path)
- Electrical safety + EMC: 6-8 weeks
- Software verification: 4-24 weeks (depends on LOC)
- Usability testing: 8-12 weeks

**Total Project Timeline:**
- Minimum (parallel execution): 12-16 weeks (limited by sterilization)
- Typical: 6-9 months (including re-tests and iterations)
- Complex devices: 12-18 months

**Critical Success Factors:**
- Early lab engagement (reserve capacity)
- Parallel test execution where possible
- Budget 20-30% contingency for re-testing
- Sterilization validation is often critical path

### Recommended Action for YOUR Device

**Before budgeting/planning:**
1. Identify specific standards from device-specific guidance (use `/fda:guidance`)
2. Request formal quotes from 3+ ISO 17025 labs
3. Understand lab capacity and lead times
4. Build in re-test contingency (20-30%)
5. Consider Pre-Submission meeting to confirm testing strategy
6. Use `/fda:test-plan` for comprehensive testing strategy

**Resources:**
- FDA Recognized Standards: https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfStandards/
- Use `/fda:test-plan` for risk-based testing matrix
- Use `/fda:guidance` to extract testing requirements from guidance

---

## Phase 2 Intelligence Features

This report includes:
- ✅ **Predicate Clinical History Assessment** - Analyzes whether predicates had clinical data (NOT predictions about YOUR device)
- ✅ **Standards Determination Guidance** - Provides process for identifying applicable standards (manual review required, NOT predictions)
- ✅ **Predicate Acceptability Assessment** - Professional SE framework evaluation per FDA guidance (2014)

**Next Phase (Phase 3):** Advanced Analytics
- MAUDE event contextualization (peer comparison)
- Review time predictions (ML-based forecasting)
- Competitive intelligence scoring (market concentration analysis)

---

**Report Version:** Phase 2.0
**Data Source:** openFDA API v2.1 + FDA Recognized Standards Database
**Analysis Date:** {datetime.now().strftime('%Y-%m-%d')}
"""

    report_path = os.path.join(project_dir, 'intelligence_report.md')
    with open(report_path, 'w') as f:
        f.write(report)

    print(f"✓ Intelligence report: {report_path}")

# ====================
# End Phase 2 Functions
# ====================

# Read CSV
rows = []
with open(CSV_PATH, 'r') as f:
    reader = csv.DictReader(f)
    rows = list(reader)

print(f"Enriching {len(rows)} devices with REAL FDA data...")
print("")

# Initialize API call log for provenance tracking (Phase 1)
api_log = []

# Process each row
enriched_rows = []
for i, row in enumerate(rows):
    k_number = row['KNUMBER']
    product_code = row.get('PRODUCTCODE', '')

    print(f"[{i+1}/{len(rows)}] {k_number}...", end=' ', flush=True)

    # Track API calls with timestamps (Phase 1)
    from datetime import datetime
    import time

    # Query 1: MAUDE events (product code level)
    start_time = time.time()
    maude = get_maude_events_by_product_code(product_code)
    api_log.append({
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'endpoint': 'device/event',
        'query': f'product_code:"{product_code}"',
        'k_number': k_number,
        'success': maude['maude_productcode_5y'] not in ['N/A', '', None],
        'duration': time.time() - start_time
    })

    # Query 2: Recalls (K-number specific)
    start_time = time.time()
    recalls = get_recall_history(k_number)
    api_log.append({
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'endpoint': 'device/recall',
        'query': f'k_numbers:"{k_number}"',
        'k_number': k_number,
        'success': True,  # Always succeeds (returns 0 if no recalls)
        'duration': time.time() - start_time
    })

    # Query 3: 510(k) Validation (K-number specific)
    start_time = time.time()
    validation = get_510k_validation(k_number)
    api_log.append({
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'endpoint': 'device/510k',
        'query': f'k_number:"{k_number}"',
        'k_number': k_number,
        'success': validation['api_validated'] == 'Yes',
        'duration': time.time() - start_time
    })

    # Add enriched fields (ALL REAL DATA)
    row.update({
        # MAUDE data (product code level - NOT device specific!)
        'maude_productcode_5y': maude['maude_productcode_5y'],
        'maude_trending': maude['maude_trending'],
        'maude_recent_6m': maude['maude_recent_6m'],
        'maude_scope': maude['maude_scope'],

        # Recall data (device specific - ACCURATE)
        'recalls_total': recalls['recalls_total'],
        'recall_latest_date': recalls['recall_latest_date'],
        'recall_class': recalls['recall_class'],
        'recall_status': recalls['recall_status'],

        # Validation data (device specific - ACCURATE)
        'api_validated': validation['api_validated'],
        'decision_description': validation['decision'],
        'expedited_review_flag': validation['expedited_review'],
        'summary_type': validation['statement_or_summary'],

        # Phase 1: Provenance and Quality fields
        'enrichment_timestamp': datetime.utcnow().isoformat() + 'Z',
        'api_version': 'openFDA v2.1',
        'data_confidence': 'HIGH' if validation['api_validated'] == 'Yes' and maude['maude_productcode_5y'] not in ['N/A', '', None] else 'MEDIUM' if validation['api_validated'] == 'Yes' or maude['maude_productcode_5y'] not in ['N/A', '', None] else 'LOW'
    })

    # Calculate and add CFR citations (Phase 1)
    citations = []
    if maude['maude_productcode_5y'] not in ['N/A', '', None, 'unknown']:
        citations.append('21 CFR 803')
    if recalls['recalls_total'] > 0:
        citations.append('21 CFR 7')
    if validation['api_validated'] == 'Yes':
        citations.append('21 CFR 807')
    row['cfr_citations'] = ', '.join(citations) if citations else 'N/A'

    # Calculate guidance document count (Phase 1)
    guidance_count = 0
    if maude['maude_productcode_5y'] not in ['N/A', '', None, 'unknown']:
        guidance_count += 1  # MDR guidance
    if recalls['recalls_total'] > 0:
        guidance_count += 1  # Recall guidance
    if validation['api_validated'] == 'Yes':
        guidance_count += 1  # 510(k) guidance
    row['guidance_refs'] = guidance_count

    # Phase 2: Intelligence Layer Analysis
    # Clinical data requirements detection
    clinical = assess_predicate_clinical_history(validation, validation['decision'])
    row['predicate_clinical_history'] = clinical['predicate_clinical_history']
    row['predicate_study_type'] = clinical['predicate_study_type']
    row['predicate_clinical_indicators'] = clinical['predicate_clinical_indicators']
    row['special_controls_applicable'] = clinical['special_controls_applicable']

    # FDA standards guidance (NOT predictions - manual review required)
    device_name = row.get('DEVICENAME', '')
    standards_guidance = provide_standards_guidance(product_code, device_name)
    row['standards_determination'] = standards_guidance['standards_determination_method']
    row['fda_standards_database'] = standards_guidance['fda_standards_database']
    row['standards_guidance'] = f"Use /fda:test-plan for comprehensive testing strategy"

    # Predicate acceptability assessment
    clearance_date = row.get('DECISIONDATE', '')
    acceptability = assess_predicate_acceptability(k_number, get_recall_history, clearance_date)
    row['predicate_acceptability'] = acceptability['acceptability_status']
    row['acceptability_rationale'] = acceptability['acceptability_rationale']
    row['predicate_risk_factors'] = acceptability['predicate_risk_factors']
    row['predicate_recommendation'] = acceptability['predicate_recommendation']

    enriched_rows.append(row)

    # Show recall status if present
    if recalls['recalls_total'] > 0:
        print(f"✓ [⚠️  {recalls['recalls_total']} recalls]")
    else:
        print(f"✓")

print(f"\n✓ API enrichment complete! Generating Phase 1 data integrity files...")

# Phase 1: Generate quality scores and metadata
generate_enrichment_process_report(PROJECT_DIR, enriched_rows, api_log)
write_enrichment_metadata(PROJECT_DIR, enriched_rows, api_log)
generate_regulatory_context(PROJECT_DIR)
generate_intelligence_report(PROJECT_DIR, enriched_rows)  # Phase 2

# Write enriched CSV (now includes quality scores and Phase 1+2 columns)
fieldnames = list(enriched_rows[0].keys())
with open(CSV_PATH, 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(enriched_rows)

print(f"\n✓ Enrichment complete! Added 29 columns (12 core + 6 Phase 1 + 11 Phase 2)")
print(f"")
print(f"  Core Enrichment Columns (12):")
print(f"  - maude_productcode_5y (⚠️  PRODUCT CODE level, not device-specific)")
print(f"  - maude_trending (increasing/decreasing/stable)")
print(f"  - maude_recent_6m (last 6 months)")
print(f"  - maude_scope (PRODUCT_CODE or UNAVAILABLE)")
print(f"  - recalls_total (✓ DEVICE SPECIFIC)")
print(f"  - recall_latest_date")
print(f"  - recall_class (I/II/III)")
print(f"  - recall_status")
print(f"  - api_validated (Yes/No)")
print(f"  - decision_description")
print(f"  - expedited_review_flag (Y/N)")
print(f"  - summary_type (Summary/Statement)")
print(f"")
print(f"  Phase 1: Data Integrity Columns (6):")
print(f"  - enrichment_timestamp (ISO 8601 format)")
print(f"  - api_version (openFDA v2.1)")
print(f"  - data_confidence (HIGH/MEDIUM/LOW)")
print(f"  - enrichment_completeness_score (0-100)")
print(f"  - cfr_citations (comma-separated CFR parts)")
print(f"  - guidance_refs (count of applicable guidance docs)")
print(f"")
print(f"  Phase 2: Intelligence Layer Columns (11):")
print(f"  - predicate_clinical_history (YES/NO/UNKNOWN - did predicates have clinical data)")
print(f"  - predicate_study_type (premarket/postmarket/none)")
print(f"  - predicate_clinical_indicators (detected in predicate clearance)")
print(f"  - special_controls_applicable (YES/NO)")
print(f"  - standards_determination (MANUAL_REVIEW_REQUIRED)")
print(f"  - fda_standards_database (URL to FDA database)")
print(f"  - standards_guidance (Use /fda:test-plan command)")
print(f"  - predicate_acceptability (ACCEPTABLE/REVIEW_REQUIRED/NOT_RECOMMENDED)")
print(f"  - acceptability_rationale (specific reasons for assessment)")
print(f"  - predicate_risk_factors (recalls, age, etc.)")
print(f"  - predicate_recommendation (action to take)")

# Count recalled devices
recalled_count = sum(1 for row in enriched_rows if row['recalls_total'] > 0)

print(f"")
print(f"✓ Devices with recalls: {recalled_count}/{len(enriched_rows)}")

# Generate simple recall report (no risk scores - those are subjective)
report_path = os.path.join(PROJECT_DIR, 'enrichment_report.html')
with open(report_path, 'w') as f:
    f.write(f'''<!DOCTYPE html>
<html>
<head>
    <title>FDA Data Enrichment Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        h1 {{ color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}
        h2 {{ color: #555; margin-top: 30px; }}
        .warning {{ background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 20px 0; }}
        .info {{ background: #d1ecf1; border-left: 4px solid #17a2b8; padding: 15px; margin: 20px 0; }}
        table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #4CAF50; color: white; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        tr:hover {{ background-color: #f1f1f1; }}
        .recall-yes {{ color: #d32f2f; font-weight: bold; }}
        .recall-no {{ color: #388e3c; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>FDA 510(k) API Enrichment Report</h1>
        <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>Total Devices:</strong> {len(enriched_rows)}</p>
        <p><strong>Devices with Recalls:</strong> {recalled_count}</p>

        <div class="warning">
            <strong>⚠️  IMPORTANT DATA LIMITATIONS</strong><br>
            <ul>
                <li><strong>MAUDE Events:</strong> Counts are at PRODUCT CODE level, NOT individual device level.
                    Use for category-level safety intelligence only.</li>
                <li><strong>Recalls:</strong> Device-specific and accurate (linked by K-number).</li>
                <li><strong>Validation:</strong> Confirms K-number exists in FDA database.</li>
            </ul>
        </div>

        <div class="info">
            <strong>ℹ️  Data Sources (All Real FDA Data)</strong><br>
            <ul>
                <li>openFDA /device/event - MAUDE adverse event reports</li>
                <li>openFDA /device/recall - Device recall database</li>
                <li>openFDA /device/510k - 510(k) clearance metadata</li>
            </ul>
        </div>

        <div class="info" style="background: #d4edda; border-left: 4px solid #28a745;">
            <strong>✓ Phase 1 & 2: Professional RA Intelligence</strong><br>
            This enrichment includes advanced features for critical regulatory professionals:
            <ul>
                <li><strong>enrichment_process_report.md</strong> - Enrichment data completeness scoring (0-100) and process validation</li>
                <li><strong>enrichment_metadata.json</strong> - Complete provenance tracking (source, timestamp, confidence)</li>
                <li><strong>regulatory_context.md</strong> - CFR citations, guidance references, and proper use guidelines</li>
                <li><strong>intelligence_report.md</strong> - Strategic insights (clinical data, standards, predicate risks)</li>
                <li><strong>CSV columns</strong> - 29 total columns (12 core + 6 Phase 1 + 11 Phase 2 intelligence)</li>
            </ul>
            <strong>Phase 2 Intelligence Features:</strong>
            <ul>
                <li>Clinical data requirements detection (YES/PROBABLE/UNLIKELY)</li>
                <li>FDA recognized consensus standards analysis (ISO 10993, IEC 60601, etc.)</li>
                <li>Predicate chain validation (HEALTHY/CAUTION/TOXIC)</li>
                <li>Resource planning & timeline estimates</li>
            </ul>
            See these files in the project directory for detailed strategic analysis.
        </div>

        <h2>Devices with Recalls (Detailed View)</h2>
''')

    if recalled_count > 0:
        f.write('''        <table>
            <tr>
                <th>K-Number</th>
                <th>Applicant</th>
                <th>Device Name</th>
                <th>Recalls</th>
                <th>Latest Recall Date</th>
                <th>Recall Class</th>
                <th>Status</th>
            </tr>
''')
        for row in enriched_rows:
            if row['recalls_total'] > 0:
                f.write(f'''            <tr>
                <td><strong>{row['KNUMBER']}</strong></td>
                <td>{row['APPLICANT']}</td>
                <td>{row.get('DEVICENAME', 'N/A')[:60]}</td>
                <td class="recall-yes">{row['recalls_total']}</td>
                <td>{row['recall_latest_date']}</td>
                <td>{row['recall_class']}</td>
                <td>{row['recall_status']}</td>
            </tr>
''')
        f.write('''        </table>''')
    else:
        f.write('<p><strong>✓ No recalls found for any device in this dataset.</strong></p>')

    f.write('''
    </div>
</body>
</html>''')

print(f"")
print(f"✓ Enrichment report (HTML): {report_path}")
print(f"")
print(f"📊 Phase 1: Data Integrity Files Generated")
print(f"  - enrichment_process_report.md: Enrichment data completeness scoring and process validation")
print(f"  - enrichment_metadata.json: Full provenance tracking")
print(f"  - regulatory_context.md: CFR citations and guidance references")
print(f"")
print(f"🧠 Phase 2: Intelligence Layer Files Generated")
print(f"  - intelligence_report.md: Strategic regulatory insights")
print(f"    • Clinical data requirements detection")
print(f"    • FDA recognized standards analysis")
print(f"    • Predicate chain validation")
print(f"    • Resource planning & timeline estimates")
print(f"")
print(f"All enriched data is traceable, validated, regulation-linked, and strategically analyzed.")

ENRICH_EOF

EXIT_CODE=$?

if [ $EXIT_CODE -eq 0 ]; then
    echo ""
    echo "✓ API enrichment complete!"
    echo "  - Added 11 intelligence columns to CSV"
    echo "  - Risk dashboard: risk_analysis.html"
else
    echo ""
    echo "⚠️  Enrichment failed (exit code: $EXIT_CODE)"
    echo "  - Basic CSV data is still available"
    echo "  - Check API key and connectivity"
fi
```

### 5.5 Display Enrichment Results

```bash
# Count recalled devices
RECALLED=$(python3 -c "
import csv
with open('$PROJECTS_DIR/$PROJECT_NAME/510k_download.csv') as f:
    reader = csv.DictReader(f)
    count = sum(1 for row in reader if int(row.get('recalls_total', 0)) > 0)
    print(count)
")

echo ""
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo "  API ENRICHMENT COMPLETE"
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo ""
echo "Added 12 columns (all real FDA data):"
echo "  ✓ MAUDE events (⚠️  product code level)"
echo "  ✓ Event trending (increasing/decreasing)"
echo "  ✓ Recall history (✓ device-specific)"
echo "  ✓ K-number validation"
echo "  ✓ Statement/Summary type"
echo ""
echo "Devices with recalls: $RECALLED"
echo "Total columns now: 36 (was 24)"
echo ""
echo "📄 Reports generated:"
echo "  - enrichment_report.html"
echo ""
```

---

## Step 6: Summary & Next Steps

### 5.1 Display Results Summary

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  FDA 510(k) Batch Fetch Complete
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PROJECT: KGN_2020-2025
────────────────────────────────────────
  Path:     ~/fda-510k-data/projects/KGN_2020-2025/

  Records:  847 submissions
  PDFs:     847 downloaded (100%)
  Failed:   0

  Date Range:   2020-01-15 to 2025-11-30
  Avg Review:   142 days

FILES CREATED
────────────────────────────────────────
  ✓ query.json                  Filter metadata
  ✓ 510k_download.csv           Submission metadata (847 rows)
  ✓ 510ks/                      Downloaded PDFs (847 files, ~4.2 GB)
  ✓ fda_data/                   FDA database archives

NEXT STEPS
────────────────────────────────────────
  1. Extract predicates from PDFs:
     /fda:extract stage2 --project KGN_2020-2025

  2. Review and score predicates:
     /fda:review --project KGN_2020-2025

  3. Draft submission sections:
     /fda:draft --project KGN_2020-2025

OPTIONAL ANALYSIS
────────────────────────────────────────
  • View statistics:        /fda:analyze download --project KGN_2020-2025
  • Gap analysis:           /fda:gap-analysis --project KGN_2020-2025
  • Safety intelligence:    /fda:safety --product-code KGN
  • Competitive analysis:   Open Applicant_ProductCode_Tables.xlsx

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Data collection complete. Ready for predicate extraction.
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 5.2 Generate Excel Analytics (If Requested)

If `--save-excel` flag was set:

```bash
# Batchfetch.py automatically generates Excel if --save-excel is passed
# Check if file exists
if [ -f "$PROJECTS_DIR/$PROJECT_NAME/Applicant_ProductCode_Tables.xlsx" ]; then
    echo ""
    echo "📊 Excel Analytics:"
    echo "   $PROJECTS_DIR/$PROJECT_NAME/Applicant_ProductCode_Tables.xlsx"
    echo "   Contains:"
    echo "   • Applicant ranking by submission count"
    echo "   • Product code distribution"
    echo "   • Timeline analysis"
    echo "   • Review time statistics"
fi
```

---

## Error Handling & Edge Cases

### No Results Found

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  No Results Found
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

APPLIED FILTERS
────────────────────────────────────────
  Date Range:   pmn96cur
  Years:        2024
  Product Code: INVALID
  Applicants:   NONEXISTENT COMPANY

ISSUE
────────────────────────────────────────
  No 510(k) submissions matched your filters.

SUGGESTIONS
────────────────────────────────────────
  1. Verify product code is correct:
     • Check spelling (e.g., KGN not KNG)
     • Search: /fda:validate --search "wound dressing"

  2. Expand date range:
     • Try removing year filter
     • Include earlier date ranges (pmn9195)

  3. Remove restrictive filters:
     • Remove applicant filter
     • Try "All decision types"

  4. Check product code exists:
     • Some codes are obsolete or merged
     • Use /fda:status to check database

TRY AGAIN
────────────────────────────────────────
  /fda:batchfetch --product-codes KGN --years 2020-2025

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

Use `AskUserQuestion` to offer:
- "Refine filters and try again"
- "Search for correct product code"
- "Cancel operation"

### Invalid Product Code

```bash
# Check product code validity
FOIACLASS="$FDA_PLUGIN_ROOT/data/foiaclass.txt"
for CODE in $(echo $PRODUCT_CODES | tr ',' ' '); do
    if ! grep -q "^$CODE|" "$FOIACLASS"; then
        echo "⚠️  Warning: Product code '$CODE' not found in FDA database"
        echo ""
        echo "Did you mean one of these?"
        grep -i "$CODE" "$FOIACLASS" | head -5 | awk -F'|' '{print "  " $1 " - " $2}'
        echo ""

        # Ask to continue or correct
        # Use AskUserQuestion
    fi
done
```

### Download Interruption

If download is interrupted (Ctrl+C, network failure, etc.):

```bash
# batchfetch.py creates download_progress.json checkpoint
# Contains list of successfully downloaded K-numbers

echo "Download interrupted!"
echo ""
echo "Resume from checkpoint:"
echo "  /fda:batchfetch --project $PROJECT_NAME --resume"
echo ""
echo "Or start fresh (deletes existing PDFs):"
echo "  /fda:batchfetch --project $PROJECT_NAME --force"
```

### FDA Rate Limiting

If encountering rate limit errors:

```
⚠️  FDA Rate Limit Detected
────────────────────────────────────────
  Downloaded:  234/847 PDFs (28%)
  Failed:      15 rate limit errors

  The FDA server is throttling requests. This is normal
  for large batch downloads.

OPTIONS
────────────────────────────────────────
  1. Resume with longer delay (Recommended):
     /fda:batchfetch --project $PROJECT_NAME --resume --delay 60

  2. Wait and retry later:
     Try again in 1-2 hours

  3. Split into smaller batches:
     Download by year ranges separately

CURRENT DELAY: 30 seconds between requests
```

### Dependency Missing

```bash
# Check if required packages are installed
python3 -c "import pandas" 2>/dev/null || {
    echo "Error: Required Python package 'pandas' not found"
    echo ""
    echo "Install dependencies:"
    echo "  pip3 install pandas requests"
    echo ""
    echo "Or use requirements file:"
    echo "  pip3 install -r $FDA_PLUGIN_ROOT/requirements.txt"
    exit 1
}
```

---

## Full-Auto Mode Implementation

When `--full-auto` is specified:

```bash
# Validate all required arguments are provided
if [ -z "$PRODUCT_CODES" ]; then
    echo "Error: --full-auto requires --product-codes"
    exit 1
fi

# Apply defaults for optional arguments
DATE_RANGE="${DATE_RANGE:-pmn96cur,pmnlstmn}"
YEARS="${YEARS:-}"  # No year filter by default
COMMITTEES="${COMMITTEES:-}"  # All committees
DECISION_CODES="${DECISION_CODES:-}"  # All decisions
APPLICANTS="${APPLICANTS:-}"  # All applicants

# Auto-generate project name if not provided
if [ -z "$PROJECT_NAME" ]; then
    PROJECT_NAME=$(generate_project_name "$PRODUCT_CODES" "$YEARS")
fi

# Skip all questions, proceed directly to preview
# Then execute immediately if preview shows valid results (>0 records)
# If zero results, exit with error
```

**Full-auto example:**
```bash
/fda:batchfetch --product-codes KGN --years 2024 --full-auto
# No questions asked, uses defaults:
#   date_range: pmn96cur,pmnlstmn
#   committees: all
#   decisions: all
#   applicants: all
#   project: KGN_2024
# Previews and downloads immediately
```

---

## Integration with Existing Pipeline

### Project Compatibility

This command creates the same project structure as `/fda:extract`, ensuring seamless pipeline integration:

```
~/fda-510k-data/projects/KGN_2020-2025/
├── query.json                    ← Created by /fda:batchfetch
├── 510k_download.csv            ← Created by /fda:batchfetch
├── 510ks/                        ← Created by /fda:batchfetch
│   ├── K123456.pdf
│   ├── K234567.pdf
│   └── ...
├── fda_data/                     ← Created by /fda:batchfetch
│   ├── pmn96cur.txt
│   └── pmnlstmn.txt
├── output.csv                    ← Created by /fda:extract stage2
├── supplement.csv                ← Created by /fda:extract stage2
├── pdf_data.json                 ← Created by /fda:extract stage2
├── review.json                   ← Created by /fda:review
├── drafts/                       ← Created by /fda:draft
└── estar/                        ← Created by /fda:assemble
```

### Command Chaining

Users can chain commands:

```bash
# Method 1: Separate commands
/fda:batchfetch --product-codes KGN --years 2024 --project my_device
/fda:extract stage2 --project my_device
/fda:review --project my_device --auto
/fda:draft --project my_device

# Method 2: Use existing /fda:extract (which calls batchfetch internally)
/fda:extract both --product-codes KGN --years 2024 --project my_device

# Method 3: Use /fda:pipeline for end-to-end automation
/fda:pipeline --product-codes KGN --years 2024 --full-auto
```

### Resume Across Commands

If download interrupted:

```bash
# Resume batchfetch download
/fda:batchfetch --project my_device --resume

# Then continue with extraction
/fda:extract stage2 --project my_device
```

---

## Usage Examples

### Example 1: Simple Product Code Search

```bash
/fda:batchfetch --product-codes KGN --years 2024 --quick
```
- Express mode: 2 questions only
- Downloads 2024 wound dressing submissions
- Project auto-named: `KGN_2024`

### Example 2: Multi-Code Historical Analysis

```bash
/fda:batchfetch --product-codes KGN,FRO,DQY --years 2015-2025
```
- Full interactive mode
- Multiple product codes
- 10-year analysis
- AI guides through all 7 filter questions

### Example 3: Competitive Intelligence

```bash
/fda:batchfetch --product-codes KGN --applicants "SMITH & NEPHEW;3M;MOLNLYCKE" --years 2020-2025 --project competitors_analysis
```
- Tracks specific companies
- Named project for organization
- Can add `--save-excel` for analytics

### Example 4: Full-Auto Batch

```bash
/fda:batchfetch --product-codes KGN --years 2024 --full-auto --project test_run --no-download
```
- No questions asked
- CSV only, no PDFs
- Fast preview run

### Example 5: Comprehensive Download

```bash
/fda:batchfetch --product-codes OVE,KWP,KWQ --date-range pmn96cur,pmnlstmn --committees OR --save-excel
```
- Orthopedic devices
- All recent data
- Committee filter
- Generate analytics workbook

---

## Tips & Best Practices

**For predicate searches:**
1. Start with last 5 years of data (recent predicates are better)
2. Use your specific product code(s) only
3. Include all decision types (even SESU/SESP may be valid predicates)
4. Don't filter by applicant (you want maximum predicate pool)

**For competitive intelligence:**
1. Filter by applicant company
2. Expand years to see trends (5-10 years)
3. Use `--save-excel` to analyze trends
4. Consider multiple product codes for full portfolio

**For market research:**
1. Use broad filters (all committees, all years)
2. Download CSV only first (`--no-download`)
3. Analyze metadata before committing to full download
4. Use `--save-excel` for market statistics

**To minimize download time:**
1. Filter by recent years (2-3 years max for targeted searches)
2. Avoid downloads over 500 PDFs if possible
3. Use `--quick` mode to skip unnecessary filters
4. Run during off-peak hours for better FDA server performance

**For large downloads (>500):**
1. Split into multiple projects by year
2. Use resume feature if interrupted
3. Monitor disk space
4. Increase `--delay` if hitting rate limits (use 60-90 seconds)

---

## Troubleshooting

**"Could not locate FDA Predicate Assistant plugin"**
- Run: `ls ~/.claude/plugins/installed_plugins.json`
- Check plugin is installed and enabled
- Try: `/fda:status` to verify installation

**"No records matched your filters"**
- Verify product code spelling
- Remove year filter
- Remove restrictive filters (applicants, decision codes)
- Try: `/fda:validate --search "device name"` to find correct code

**"Rate limit errors during download"**
- Use `--resume` to continue from checkpoint
- Increase delay: `--delay 60`
- Split into smaller batches by year
- Try again during off-peak hours (evenings/weekends)

**"Low disk space warning"**
- Check available space: `df -h ~`
- Each PDF is ~5MB average
- Use `--no-download` for CSV-only if disk limited
- Move projects to external drive if needed

**"Python package not found"**
- Install: `pip3 install pandas requests`
- Or: `pip3 install -r $FDA_PLUGIN_ROOT/requirements.txt`
- Check Python version: `python3 --version` (need 3.7+)

---

This command provides a **collaborative, AI-guided experience** for FDA data collection while leveraging the battle-tested `batchfetch.py` script for actual execution. The interactive workflow makes complex filtering accessible to non-technical users while providing expert context at every decision point.
