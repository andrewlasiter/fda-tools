# TICKET-001 Data Pipeline Audit Report
**Date:** 2026-02-15
**Commit:** f82825d
**Pipeline:** presub.md ‚Üí presub_metadata.json ‚Üí estar_xml.py ‚Üí presub_prestar.xml
**Reviewer:** Data Engineer
**Status:** üü° CONDITIONAL PASS (3 Critical Issues, 8 Recommendations)

---

## Executive Summary

The TICKET-001 Pre-Submission data pipeline implements a **5-stage transformation** from user input through FDA-compliant PreSTAR XML. The architecture follows a **Pragmatic Balance** pattern with intelligent question selection, template population, and metadata-driven XML generation.

**Overall Assessment:** The pipeline is functionally complete but has **3 critical data integrity risks** and **8 optimization opportunities** identified below.

---

## Pipeline Architecture

```
Stage 1: User Input + API Data
   ‚îú‚îÄ Product code (required)
   ‚îú‚îÄ Device description (optional)
   ‚îú‚îÄ Intended use (optional)
   ‚îî‚îÄ Meeting type override (optional)

Stage 2: Question Selection (presub.md Step 3.5)
   ‚îú‚îÄ Load presub_questions.json (34 questions)
   ‚îú‚îÄ Get meeting_type_defaults
   ‚îú‚îÄ Check auto_triggers against device_description keywords
   ‚îú‚îÄ Sort by priority (descending)
   ‚îú‚îÄ Limit to top 7 questions
   ‚îî‚îÄ Export QUESTION_IDS, QUESTION_COUNT

Stage 3: Template Population (presub.md Step 4.1)
   ‚îú‚îÄ Load template file from data/templates/presub_meetings/{TEMPLATE_FILE}
   ‚îú‚îÄ Populate 80+ {placeholder} variables
   ‚îú‚îÄ Replace with project data (classification, review, guidance)
   ‚îî‚îÄ Write presub_plan.md

Stage 4: Metadata Generation (presub.md Step 6)
   ‚îú‚îÄ Collect all pipeline variables
   ‚îú‚îÄ Format presub_metadata.json with:
   ‚îÇ  ‚îú‚îÄ meeting_type, detection_method, detection_rationale
   ‚îÇ  ‚îú‚îÄ questions_generated (array of question IDs)
   ‚îÇ  ‚îú‚îÄ question_count, template_used
   ‚îÇ  ‚îî‚îÄ auto_triggers_fired, data_sources_used
   ‚îî‚îÄ Write to project_dir/presub_metadata.json

Stage 5: XML Generation (estar_xml.py _build_prestar_xml)
   ‚îú‚îÄ Load presub_metadata.json
   ‚îú‚îÄ Load presub_questions.json question bank
   ‚îú‚îÄ Map question IDs ‚Üí full question text
   ‚îú‚îÄ Format SCTextField110 (submission characteristics)
   ‚îú‚îÄ Format QPTextField110 (questions text)
   ‚îî‚îÄ Generate presub_prestar.xml with populated fields
```

---

## Critical Issues

### üî¥ CRITICAL-1: Missing Data Validation (Severity: HIGH)

**Location:** `presub.md` Step 3.5, lines 274-336
**Issue:** Question selection pipeline has **no validation** that `presub_questions.json` exists or is valid JSON before attempting to load it.

**Code:**
```python
question_bank_path = os.path.join(os.environ.get("FDA_PLUGIN_ROOT", ""), "data/question_banks/presub_questions.json")
if not os.path.exists(question_bank_path):
    print("QUESTION_BANK_MISSING")
    sys.exit(1)

with open(question_bank_path) as f:
    question_bank = json.load(f)  # ‚ùå No try/except - corrupt JSON causes pipeline crash
```

**Impact:**
- Corrupt `presub_questions.json` ‚Üí Python crash ‚Üí **no error recovery**
- User sees raw Python traceback instead of actionable error message
- Downstream stages (metadata, XML) fail silently with empty question data

**Risk Scenario:**
1. User hand-edits `presub_questions.json` and introduces syntax error
2. Pipeline crashes at Step 3.5 with `JSONDecodeError`
3. `presub_metadata.json` generated with empty `questions_generated: []`
4. PreSTAR XML exports with empty `QPTextField110` field
5. FDA eSTAR import succeeds but submission has **no questions** ‚Üí RTA

**Recommendation:**
```python
try:
    with open(question_bank_path) as f:
        question_bank = json.load(f)
except json.JSONDecodeError as e:
    print(f"ERROR: Question bank JSON is corrupt: {e}")
    print(f"File: {question_bank_path}")
    print("Fix the JSON syntax or restore from git.")
    sys.exit(1)
except Exception as e:
    print(f"ERROR: Cannot load question bank: {e}")
    sys.exit(1)
```

---

### üî¥ CRITICAL-2: Schema Inconsistency Between Generation and Consumption

**Location:** `presub.md` Step 6 (lines 1375-1417) vs `estar_xml.py` _collect_project_values (lines 772-952)
**Issue:** `presub_metadata.json` schema has **no version check** ‚Äî if schema changes, XML generation silently fails with missing fields.

**Generation Schema (presub.md):**
```json
{
  "version": "1.0",
  "meeting_type": "formal",
  "questions_generated": ["PRED-001", "CLASS-001"],
  "question_count": 2,
  "template_used": "formal_meeting.md",
  "auto_triggers_fired": ["patient_contacting", "sterile_device"]
}
```

**Consumption Code (estar_xml.py):**
```python
presub_metadata = project_data.get("presub_metadata", {})  # No version check!
questions_generated = presub_metadata.get("questions_generated", [])  # Defaults to []
```

**Impact:**
- If future version adds required fields (e.g., `meeting_date`, `fda_division`), old metadata files silently produce incomplete XML
- No schema migration path ‚Üí users must manually regenerate metadata
- **Data loss risk:** Old metadata gets populated into XML with missing critical fields

**Risk Scenario:**
1. v2.0 adds `"fda_division": "CDRH/DHT1A"` to schema
2. estar_xml.py v2.0 expects this field for `<ReviewDivision>` XML tag
3. User has v1.0 `presub_metadata.json` from earlier run
4. XML generation reads v1.0 file, `fda_division` missing
5. XML exports with empty `<ReviewDivision>` tag
6. FDA eSTAR import fails validation ‚Üí submission rejected

**Recommendation:**
```python
# In estar_xml.py _collect_project_values
presub_metadata = project_data.get("presub_metadata", {})
metadata_version = presub_metadata.get("version", "0.0")

if metadata_version != "1.0":
    print(f"WARNING: presub_metadata.json version {metadata_version} may be incompatible with this tool.")
    print("Expected version: 1.0")
    print("Regenerate metadata with: /fda:presub --project NAME")
    # Continue with best-effort parsing, but warn user
```

---

### üî¥ CRITICAL-3: Keyword Matching Has No Stemming or Fuzzy Logic

**Location:** `presub.md` Step 3.5, lines 296-309
**Issue:** Auto-trigger keyword matching uses **exact substring match** ‚Äî fails on synonyms, plurals, or spelling variations.

**Code:**
```python
device_keywords = {
    "patient_contacting": ["patient contact", "contacting", "skin contact", "tissue contact"],
    "sterile_device": ["sterile", "sterilization", "sterilized"],
    "powered_device": ["powered", "electrical", "electronic", "battery"],
}

for trigger_name, keywords in device_keywords.items():
    if any(kw in device_description for kw in keywords):  # ‚ùå Exact substring match only
        triggered_ids = auto_triggers.get(trigger_name, [])
        selected_ids.update(triggered_ids)
```

**Impact:**
- **False Negatives:**
  - "Skin-contacting" ‚Üí missed (hyphenated)
  - "EtO sterilisation" ‚Üí missed (British spelling)
  - "Battery-powered" ‚Üí missed (hyphenated)
  - "Electrically-powered" ‚Üí missed (adverb form)
- **Result:** Critical testing questions missing from Pre-Sub ‚Üí FDA RTA

**Risk Scenario:**
1. Device description: "EtO-sterilised surgical retractor"
2. Keyword list has "sterilized" (American spelling)
3. Pattern match fails ‚Üí `sterile_device` auto-trigger NOT fired
4. Question `TEST-STER-001` (sterilization validation) NOT selected
5. Pre-Sub submitted **without sterilization question**
6. FDA reviewer: "Why no sterilization discussion?" ‚Üí Deficiency letter

**Test Cases Missing:**
```python
# Should match but DON'T:
"skin-contacting polymer"        # hyphenated
"EO sterilisation"               # British spelling
"electrically powered"           # adverb
"rechargeable battery system"   # synonym
```

**Recommendation:**
```python
import re

def normalize_text(text):
    """Normalize for fuzzy keyword matching."""
    text = text.lower()
    text = re.sub(r'[-/]', ' ', text)  # "skin-contacting" ‚Üí "skin contacting"
    text = re.sub(r'isation\b', 'ization', text)  # British ‚Üí American
    text = re.sub(r'ised\b', 'ized', text)
    return text

device_description_normalized = normalize_text(device_description)

for trigger_name, keywords in device_keywords.items():
    keywords_normalized = [normalize_text(kw) for kw in keywords]
    if any(kw in device_description_normalized for kw in keywords_normalized):
        triggered_ids = auto_triggers.get(trigger_name, [])
        selected_ids.update(triggered_ids)
```

---

## Data Flow Breaks

### ‚ö†Ô∏è BREAK-1: Empty QUESTION_IDS Propagates Through Pipeline

**Location:** Entire pipeline
**Issue:** If question selection produces zero questions, downstream stages don't fail ‚Äî they generate valid but **empty** output.

**Flow:**
```
Step 3.5: QUESTION_IDS="[]" (empty)
   ‚Üì
Step 4.1: auto_generated_questions="" (empty string)
   ‚Üì
Step 6: "questions_generated": [] (valid JSON)
   ‚Üì
estar_xml.py: presub_questions="" (valid XML, empty field)
   ‚Üì
presub_prestar.xml: <QPTextField110></QPTextField110> (valid but useless)
```

**Impact:**
- No error raised ‚Üí user doesn't know questions are missing
- FDA submission **appears complete** but has empty questions section
- Likely RTA: "Pre-Sub must include specific questions for FDA review"

**Recommendation:**
Add validation checkpoint at Step 6:
```python
if question_count == 0 and meeting_type in ["formal", "written"]:
    print("ERROR: No questions generated for meeting type '{meeting_type}'")
    print("Formal and written Pre-Subs require at least 1 question.")
    print("Check device_description triggers or add questions manually.")
    sys.exit(1)
```

---

### ‚ö†Ô∏è BREAK-2: Placeholder Replacement Has No Completeness Check

**Location:** `presub.md` Step 4.1, lines 431-642
**Issue:** Template population replaces `{placeholder}` variables but **doesn't track** which ones failed to populate.

**Code:**
```python
for key, value in placeholders.items():
    placeholder_pattern = '{' + key + '}'
    populated_content = populated_content.replace(placeholder_pattern, str(value))
    # ‚ùå No tracking if value was empty string or "[TODO: ...]"
```

**Impact:**
- User gets `presub_plan.md` with mix of populated and `[TODO: ...]` fields
- No summary report: "32 of 80 placeholders populated"
- User must manually grep for `[TODO` to find missing data

**Recommendation:**
```python
populated_count = 0
placeholder_count = len(placeholders)

for key, value in placeholders.items():
    placeholder_pattern = '{' + key + '}'
    if value and not value.startswith("[TODO"):
        populated_count += 1
    populated_content = populated_content.replace(placeholder_pattern, str(value))

print(f"Template population: {populated_count}/{placeholder_count} fields ({populated_count*100//placeholder_count}%)")
if populated_count < placeholder_count * 0.5:
    print("WARNING: Less than 50% of placeholders populated. Run /fda:review and /fda:guidance first.")
```

---

## Edge Cases and Missing Transformations

### üü° EDGE-1: Question Priority Ranking Ignores Meeting Type Applicability

**Location:** `presub.md` Step 3.5, lines 316-318
**Issue:** Questions sorted by `priority` but NOT filtered by `applicable_meeting_types` **before** sorting.

**Code:**
```python
selected_questions = [q for q in questions if q.get("id") in selected_ids]
selected_questions.sort(key=lambda q: q.get("priority", 0), reverse=True)  # Sort AFTER filtering
```

**Expected Behavior:**
1. Filter questions by `applicable_meeting_types` matching `meeting_type`
2. THEN sort by priority
3. THEN limit to 7

**Actual Behavior:**
1. Get questions from `selected_ids` (already filtered by auto-triggers)
2. Sort all selected questions by priority
3. Limit to 7

**Impact:**
- Low priority but highly relevant questions may be excluded
- Example: For `meeting_type="written"`, question `IFU-001` (priority 80, applicable to ["formal", "written"]) should rank higher than `NOVEL-001` (priority 90, applicable to ["formal"] only)
- Current logic: `NOVEL-001` ranks higher ‚Üí selected for written meeting even though not applicable

**Test Case:**
```python
meeting_type = "written"
questions = [
    {"id": "NOVEL-001", "priority": 90, "applicable_meeting_types": ["formal"]},
    {"id": "IFU-001", "priority": 80, "applicable_meeting_types": ["formal", "written"]},
]
# Expected: IFU-001 selected (applicable to written)
# Actual: NOVEL-001 selected (higher priority but NOT applicable)
```

**Recommendation:**
```python
# Filter by applicable meeting types FIRST
applicable_questions = [
    q for q in questions
    if q.get("id") in selected_ids
    and meeting_type in q.get("applicable_meeting_types", [])
]

# THEN sort by priority
applicable_questions.sort(key=lambda q: q.get("priority", 0), reverse=True)

# THEN limit to 7
selected_questions = applicable_questions[:7]
```

---

### üü° EDGE-2: Missing Type Checking on presub_metadata Fields

**Location:** `estar_xml.py` lines 843-896
**Issue:** PreSTAR XML generation assumes `questions_generated` is a list but **doesn't validate** type.

**Code:**
```python
questions_generated = presub_metadata.get("questions_generated", [])  # Assumes list
if questions_generated and question_bank:
    for q_id in questions_generated:  # ‚ùå Crashes if questions_generated is a string
```

**Impact:**
- If `presub_metadata.json` manually edited with `"questions_generated": "PRED-001,CLASS-001"` (string instead of list)
- Python crashes: `TypeError: 'str' object is not iterable`

**Recommendation:**
```python
questions_generated = presub_metadata.get("questions_generated", [])

# Type validation
if not isinstance(questions_generated, list):
    print(f"WARNING: questions_generated should be a list, got {type(questions_generated).__name__}")
    # Attempt recovery: convert string to list
    if isinstance(questions_generated, str):
        questions_generated = questions_generated.split(",")
    else:
        questions_generated = []
```

---

### üü° EDGE-3: No Handling for Duplicate Question IDs

**Location:** `presub.md` Step 3.5, lines 291-318
**Issue:** `selected_ids` is a **set** (good!), but if question bank has duplicate IDs, only one question text is retrieved.

**Code:**
```python
selected_ids = set(default_question_ids)  # Good: set prevents duplicates
# But what if presub_questions.json has duplicate IDs?
questions = question_bank.get("questions", [])
selected_questions = [q for q in questions if q.get("id") in selected_ids]  # First match wins
```

**Impact:**
- If `presub_questions.json` has:
  ```json
  [
    {"id": "PRED-001", "text": "Old version..."},
    {"id": "PRED-001", "text": "Updated version..."}
  ]
  ```
- Only first match is selected ‚Üí user gets outdated question text

**Recommendation:**
Add duplicate detection during question bank load:
```python
with open(question_bank_path) as f:
    question_bank = json.load(f)

# Validate no duplicate IDs
question_ids = [q.get("id") for q in question_bank.get("questions", [])]
duplicates = [qid for qid in set(question_ids) if question_ids.count(qid) > 1]
if duplicates:
    print(f"ERROR: Question bank has duplicate IDs: {duplicates}")
    print("Fix presub_questions.json before proceeding.")
    sys.exit(1)
```

---

## Data Integrity Recommendations

### üìä REC-1: Add Schema Validation for presub_metadata.json

**Priority:** High
**Effort:** 2 hours

Implement JSON Schema validation in both generation (Step 6) and consumption (estar_xml.py):

```python
# schemas/presub_metadata_schema.json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["version", "meeting_type", "questions_generated"],
  "properties": {
    "version": {"type": "string", "pattern": "^\\d+\\.\\d+$"},
    "meeting_type": {"type": "string", "enum": ["formal", "written", "info", "pre-ide", "administrative", "info-only"]},
    "questions_generated": {
      "type": "array",
      "items": {"type": "string", "pattern": "^[A-Z]+-\\d{3}$"}
    },
    "question_count": {"type": "integer", "minimum": 0, "maximum": 20}
  }
}
```

Validation code:
```python
import jsonschema

schema_path = os.path.join(FDA_PLUGIN_ROOT, "schemas/presub_metadata_schema.json")
with open(schema_path) as f:
    schema = json.load(f)

try:
    jsonschema.validate(metadata, schema)
except jsonschema.ValidationError as e:
    print(f"ERROR: presub_metadata.json is invalid: {e.message}")
    sys.exit(1)
```

---

### üìä REC-2: Implement Data Provenance Tracking

**Priority:** Medium
**Effort:** 3 hours

Track which data sources contributed to each field in `presub_metadata.json`:

```json
{
  "version": "1.0",
  "questions_generated": ["PRED-001", "CLASS-001"],
  "data_provenance": {
    "questions_generated": {
      "source": "auto_triggers",
      "triggers_fired": ["patient_contacting", "sterile_device"],
      "default_questions": ["CLASS-001"],
      "timestamp": "2026-02-15T10:30:00Z"
    },
    "device_description": {
      "source": "user_input",
      "argument": "--device-description",
      "timestamp": "2026-02-15T10:25:00Z"
    },
    "meeting_type": {
      "source": "auto_detection",
      "detection_method": "question_count_based",
      "rationale": "4 questions ‚Üí formal meeting recommended",
      "timestamp": "2026-02-15T10:28:00Z"
    }
  }
}
```

**Value:**
- Audit trail for compliance review
- Debugging: "Why was question PRED-002 not selected?"
- Reproducibility: Re-run with same inputs

---

### üìä REC-3: Add Checksum Validation for Question Bank

**Priority:** Medium
**Effort:** 1 hour

Generate SHA-256 checksum of `presub_questions.json` and store in metadata:

```python
import hashlib

def checksum_file(filepath):
    sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256.update(chunk)
    return sha256.hexdigest()

question_bank_checksum = checksum_file(question_bank_path)
metadata["question_bank_checksum"] = question_bank_checksum
metadata["question_bank_version"] = question_bank.get("version", "1.0")
```

**Value:**
- Detect if question bank changed between metadata generation and XML export
- Warning if checksums mismatch: "Question bank modified since metadata was generated. Regenerate metadata to ensure consistency."

---

### üìä REC-4: Implement Graceful Degradation for Missing Data

**Priority:** Medium
**Effort:** 2 hours

Currently, missing placeholders get replaced with empty strings. Instead, categorize by severity:

```python
PLACEHOLDER_SEVERITY = {
    "product_code": "CRITICAL",  # Required for FDA submission
    "device_description": "CRITICAL",
    "indications_for_use": "CRITICAL",
    "applicant_name": "HIGH",  # Required but can be filled manually
    "contact_email": "HIGH",
    "predicate_k_number": "MEDIUM",  # Optional for some meeting types
    "feature_1": "LOW",  # Nice-to-have details
}

missing_critical = []
missing_high = []

for key, value in placeholders.items():
    if not value or value.startswith("[TODO"):
        severity = PLACEHOLDER_SEVERITY.get(key, "LOW")
        if severity == "CRITICAL":
            missing_critical.append(key)
        elif severity == "HIGH":
            missing_high.append(key)

if missing_critical:
    print(f"ERROR: Critical placeholders missing: {missing_critical}")
    print("Provide these arguments: --device-description, --intended-use, --product-code")
    sys.exit(1)

if missing_high:
    print(f"WARNING: Important placeholders missing: {missing_high}")
    print("Output will need manual completion before FDA submission.")
```

---

### üìä REC-5: Add XML Field Population Metrics to Metadata

**Priority:** Low
**Effort:** 1 hour

Track which XML fields were successfully populated:

```json
{
  "xml_generation": {
    "fields_total": 18,
    "fields_populated": 14,
    "fields_empty": 4,
    "empty_fields": ["ADTextField220", "ADTextField240", "ADTextField250", "ADTextField260"],
    "population_rate": 0.78
  }
}
```

**Value:**
- Quality metrics: "78% of PreSTAR fields populated from project data"
- Identify which project data files are missing (e.g., no address ‚Üí import_data.json missing applicant section)

---

### üìä REC-6: Implement Idempotency Check for Metadata Generation

**Priority:** Low
**Effort:** 1 hour

Detect if `presub_metadata.json` already exists and warn user:

```python
metadata_path = os.path.join(project_dir, "presub_metadata.json")
if os.path.exists(metadata_path):
    with open(metadata_path) as f:
        existing = json.load(f)

    print(f"WARNING: presub_metadata.json already exists (generated {existing.get('generated_at')})")
    print("Overwrite? [y/N]: ", end="")
    response = input().strip().lower()
    if response != 'y':
        print("Metadata generation cancelled. Use existing file.")
        return
    print("Overwriting existing metadata...")
```

**Value:**
- Prevent accidental re-generation with different parameters
- Preserve manual edits to metadata

---

### üìä REC-7: Add Logging for Auto-Trigger Decisions

**Priority:** Low
**Effort:** 1 hour

Log which keywords matched for each auto-trigger:

```python
trigger_log = []

for trigger_name, keywords in device_keywords.items():
    matched_keywords = [kw for kw in keywords if kw in device_description]
    if matched_keywords:
        triggered_ids = auto_triggers.get(trigger_name, [])
        selected_ids.update(triggered_ids)
        trigger_log.append({
            "trigger": trigger_name,
            "matched_keywords": matched_keywords,
            "questions_added": triggered_ids
        })

metadata["auto_trigger_log"] = trigger_log
```

**Value:**
- Debugging: "Why was TEST-BIO-001 selected?" ‚Üí "Trigger: patient_contacting, Matched: 'skin contact'"
- User transparency: Show decision rationale in metadata

---

### üìä REC-8: Implement XML Roundtrip Validation

**Priority:** Low
**Effort:** 2 hours

After generating PreSTAR XML, parse it back and validate against original metadata:

```python
# After writing presub_prestar.xml
with open(output_xml_path) as f:
    xml_content = f.read()

# Parse back
parsed_data = parse_xml_data(xml_content)

# Validate key fields
assert parsed_data["classification"]["product_code"] == metadata["product_code"]
assert len(parsed_data["raw_fields"].get("QPTextField110", "")) > 0  # Questions not empty

print("‚úì XML roundtrip validation passed")
```

**Value:**
- Detect XML generation bugs before user imports into FDA eSTAR
- Catch encoding issues (e.g., special characters corrupted)

---

## Performance and Scalability

### Current Performance Profile

Based on code analysis:

```
Step 3.5 (Question Selection):
  - JSON load: ~5ms (presub_questions.json is 10KB)
  - Keyword matching: O(Q √ó K) where Q=34 questions, K=~50 keywords ‚Üí ~10ms
  - Sorting: O(Q log Q) ‚Üí <1ms
  Total: ~20ms

Step 4.1 (Template Population):
  - Template read: ~10ms (templates are 100-200KB)
  - String replacement: O(P √ó T) where P=80 placeholders, T=template size ‚Üí ~50ms
  Total: ~60ms

Step 6 (Metadata Generation):
  - JSON serialization: ~5ms
  Total: ~5ms

Step 7 (XML Generation):
  - Project data load: ~20ms (4-5 JSON files)
  - Question bank reload: ~5ms
  - XML string building: ~30ms
  Total: ~55ms

Pipeline Total: ~140ms
```

**Bottleneck:** String replacement in Step 4.1 (43% of total time)

**Scalability:**
- Current: 34 questions, 6 templates, 80 placeholders ‚Üí **140ms**
- At 100 questions, 20 templates, 200 placeholders ‚Üí **~380ms** (still acceptable)
- Memory usage: <10MB (all in-memory JSON/string operations)

**Recommendation:** No immediate performance concerns. Consider caching loaded question bank if `/fda:presub` is called repeatedly in same session.

---

## Testing Gaps

### Unit Tests Needed

1. **Question Selection Logic:**
   - Empty question bank ‚Üí error
   - Duplicate question IDs ‚Üí error
   - Keyword matching edge cases (hyphenation, plurals, synonyms)
   - Priority ranking with meeting type filtering

2. **Placeholder Replacement:**
   - All 80 placeholders have test coverage
   - Edge case: `{placeholder}` appears in question text itself
   - Edge case: Circular references (e.g., `{device_description}` contains `{product_code}`)

3. **Metadata Schema:**
   - Valid JSON structure
   - Required fields present
   - Type validation (arrays, strings, integers)
   - Version compatibility

4. **XML Generation:**
   - Empty metadata ‚Üí minimal XML
   - Full metadata ‚Üí all fields populated
   - Special characters in text ‚Üí proper XML escaping
   - Roundtrip validation (generate ‚Üí parse ‚Üí compare)

### Integration Tests Needed

1. **End-to-End Pipeline:**
   - `/fda:presub DQY --project test --device-description "..." --intended-use "..."`
   - Verify `presub_metadata.json` exists and valid
   - Verify `presub_prestar.xml` exists and valid
   - Import XML into FDA PreSTAR template (requires Adobe Acrobat)

2. **Error Recovery:**
   - Corrupt question bank ‚Üí graceful error
   - Missing project directory ‚Üí graceful error
   - Invalid product code ‚Üí graceful error

3. **Data Source Priority:**
   - import_data.json overrides device_profile.json
   - CLI arguments override all JSON sources
   - Verify correct priority in populated output

---

## Compliance and Regulatory Impact

### FDA eSTAR Validation

**Critical:** PreSTAR XML must pass FDA eSTAR import validation. Current implementation has **no validation** against FDA XSD schema.

**Risk:**
- XML structure mismatch ‚Üí FDA eSTAR rejects import
- User discovers error only after attempting import in Adobe Acrobat
- No automated way to test XML compliance

**Recommendation:**
1. Obtain FDA PreSTAR XSD schema (if available via FOIA request)
2. Add `xmlschema` validation:
   ```python
   import xmlschema

   xsd_path = os.path.join(FDA_PLUGIN_ROOT, "schemas/FDA_5064_PreSTAR.xsd")
   schema = xmlschema.XMLSchema(xsd_path)

   if not schema.is_valid(xml_string):
       errors = schema.iter_errors(xml_string)
       print("ERROR: PreSTAR XML failed FDA validation:")
       for error in errors:
           print(f"  {error}")
       sys.exit(1)
   ```

**Workaround (if XSD unavailable):**
Test with actual FDA template:
1. Generate XML with known-good test data
2. Import into FDA PreSTAR template
3. Document any import errors
4. Update XML generation to fix

---

## Documentation Gaps

### Missing from PRESTAR_WORKFLOW.md

1. **Troubleshooting:** What if question selection produces zero questions?
2. **Manual Editing:** Can users edit `presub_metadata.json` safely? Which fields?
3. **Version Compatibility:** What if metadata was generated with older plugin version?
4. **Data Sources:** Priority order when multiple sources have same field (import_data vs device_profile vs query)
5. **Validation:** How to verify XML is FDA-compliant before submission?

### Recommended Additions

Add to `PRESTAR_WORKFLOW.md`:

```markdown
## Troubleshooting

### No Questions Generated

If `presub_metadata.json` shows `"questions_generated": []`:

1. Check device description for auto-trigger keywords:
   - "patient contact" ‚Üí biocompatibility questions
   - "sterile" ‚Üí sterilization questions
   - "software" ‚Üí software questions

2. Verify question bank is loaded:
   ```bash
   ls -lh $FDA_PLUGIN_ROOT/data/question_banks/presub_questions.json
   ```

3. Manually add questions:
   Edit `presub_metadata.json`:
   ```json
   "questions_generated": ["PRED-001", "CLASS-001", "TEST-BIO-001"]
   ```

   Then regenerate XML:
   ```bash
   /fda:presub --project NAME --generate-xml-only
   ```

### XML Import Fails in Adobe Acrobat

Common errors:

- **"Invalid field name":** Field ID mismatch ‚Üí regenerate with `--format real`
- **"Encoding error":** Special characters ‚Üí check for non-ASCII in device description
- **"Empty fields":** Metadata incomplete ‚Üí run `/fda:review` and `/fda:guidance` first
```

---

## Final Recommendations

### Immediate Actions (Critical)

1. ‚úÖ **Add try/except to question bank JSON load** (CRITICAL-1)
2. ‚úÖ **Add schema version check to estar_xml.py** (CRITICAL-2)
3. ‚úÖ **Implement keyword normalization** (CRITICAL-3)

**Effort:** 4 hours
**Risk Reduction:** Eliminates 3 critical data loss scenarios

### Short-Term (High Value)

4. ‚úÖ **Filter questions by applicable_meeting_types** (EDGE-1)
5. ‚úÖ **Add type validation for presub_metadata fields** (EDGE-2)
6. ‚úÖ **Add duplicate question ID detection** (EDGE-3)
7. ‚úÖ **Validate question_count > 0 for formal/written meetings** (BREAK-1)

**Effort:** 6 hours
**Risk Reduction:** Prevents silent failures and incorrect question selection

### Medium-Term (Data Quality)

8. ‚úÖ **Implement JSON Schema validation** (REC-1)
9. ‚úÖ **Add data provenance tracking** (REC-2)
10. ‚úÖ **Add checksum validation for question bank** (REC-3)

**Effort:** 6 hours
**Value:** Audit trail, reproducibility, compliance documentation

### Long-Term (Nice-to-Have)

11. ‚úÖ **Implement graceful degradation** (REC-4)
12. ‚úÖ **Add XML field population metrics** (REC-5)
13. ‚úÖ **Implement idempotency check** (REC-6)
14. ‚úÖ **Add auto-trigger decision logging** (REC-7)
15. ‚úÖ **Implement XML roundtrip validation** (REC-8)

**Effort:** 7 hours
**Value:** Enhanced user experience, better debugging, quality metrics

---

## Conclusion

**Status:** üü° **CONDITIONAL PASS**

The TICKET-001 data pipeline is **functionally complete** and delivers significant value (2-4 hour time savings per Pre-Sub). However, **3 critical issues** pose data integrity risks that must be addressed before production use:

1. **Missing JSON error handling** ‚Üí pipeline crashes
2. **No schema version checking** ‚Üí silent data loss
3. **Brittle keyword matching** ‚Üí missed questions

**Recommendation:** Implement the 7 immediate + short-term fixes (**10 hours total**) to achieve production readiness. The pipeline will then be:
- ‚úÖ **Robust:** Graceful error handling for all failure modes
- ‚úÖ **Reliable:** Schema validation prevents data loss
- ‚úÖ **Accurate:** Fuzzy keyword matching catches all relevant questions

**Risk Assessment:**
- **Pre-Fix:** HIGH (3 critical data loss paths)
- **Post-Fix:** LOW (normal operational risk)

**Approval:** Conditional on implementing fixes 1-7 within 2 weeks.

---

**Audit Completed:** 2026-02-15
**Next Review:** After implementing critical fixes (estimated 2026-02-22)
**Reviewer Signature:** Data Engineering Team
**Document Version:** 1.0
